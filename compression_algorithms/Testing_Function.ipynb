{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from numpy.random import rand,randint\n",
    "import scipy.ndimage\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import queue\n",
    "import functools\n",
    "import pandas\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "cluster_number = 256\n",
    "weights = rand(300,784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_weights_for_kmeans(weights):\n",
    "    return np.hstack(weights).reshape(-1,1)\n",
    "\n",
    "def build_clusters(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(reshape_weights_for_kmeans(weights))\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_centroid_index(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    return (np.abs(centers - value)).argmin()\n",
    "\n",
    "def nearest_centroid(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return centers[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_weights(weights,centers):\n",
    "    arr_ret = np.empty_like(weights).astype(np.int16)\n",
    "    for i, row in enumerate(weights):\n",
    "        for j, col in enumerate(row):\n",
    "            arr_ret[i,j] = nearest_centroid_index(centers,weights[i,j])\n",
    "    return arr_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_matrix_to_matrix(idx_matrix,centers,shape):\n",
    "    return centers[idx_matrix.reshape(-1,1)].reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_gradient_matrix(idx_matrix,gradient,cluster):\n",
    "    return scipy.ndimage.sum(gradient,idx_matrix,index=range(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map (i,j) -> k\n",
    "def dict_index_to_cluster(weights,centers):\n",
    "        dict_ret = {}\n",
    "        for i, row in enumerate(weights):\n",
    "            for j, col in enumerate(row):\n",
    "                dict_ret[(i,j)] = nearest_centroid_index(centers,weights[i,j])\n",
    "        return dict_ret\n",
    "\n",
    "# map k -> (i,j)\n",
    "def dict_cluster_to_index(dict_idx):\n",
    "    dict_ret = {}\n",
    "    for k,v in dict_idx.items():\n",
    "        if v in dict_ret:\n",
    "            dict_ret[v] += [k]\n",
    "        else:\n",
    "            dict_ret[v] = [k]\n",
    "    return dict_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_dict_to_matrix(dict_index,dict_values,shape):\n",
    "    coord_array = np.asarray(list(dict_index.values()))\n",
    "    return dict_values[coord_array].reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_gradient_dict(K_Index,W_Matrix):\n",
    "    tmpindex = dict((key, ([x for x, _ in value], [y for _, y in value])) for key, value in K_Index.items())\n",
    "    return [W_Matrix[value[0],value[1]].sum() for value in tmpindex.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = build_clusters(cluster_number,weights)\n",
    "dict_index = dict_index_to_cluster(weights,centers)\n",
    "dict_cluster = dict_cluster_to_index(dict_index)\n",
    "matrix_index = redefine_weights(weights,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit idx_matrix_to_matrix(matrix_index,centers,(300,784))\n",
    "%timeit index_dict_to_matrix(dict_index,centers,(300,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit centroid_gradient_matrix(matrix_index,weights,cluster_number)\n",
    "%timeit centroid_gradient_dict(dict_cluster,weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_index_matrix_slow(dict_cluster):\n",
    "    arr_ret = np.zeros((300,785))\n",
    "    for k,v in dict_cluster.items():\n",
    "        for i in v:\n",
    "            arr_ret[i] = k\n",
    "    return arr_ret\n",
    "\n",
    "def dict_to_index_matrix(dict_index,shape):\n",
    "    return np.asarray(list(dict_index.values())).reshape(shape).astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dict_to_index_matrix_slow(dict_cluster)\n",
    "%timeit dict_to_index_matrix(dict_index,(300,784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_distance(weights,centroids):\n",
    "    tot = 0.\n",
    "    for i, row in enumerate(weights):\n",
    "        for j, col in enumerate(row):\n",
    "            weight = weights[i,j]\n",
    "            centroid = nearest_centroid(centroids,weight)[0]\n",
    "            tot += np.sqrt((weight - centroid)**2)\n",
    "    return tot / ((i+1)*(j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_number(values,n_from,n_to,n_jump):\n",
    "    result = {}\n",
    "    for i in range(n_from,n_to+1,n_jump):\n",
    "        kmeans = MiniBatchKMeans(n_clusters=i,random_state=RANDOM_SEED)\n",
    "        kmeans.fit(reshape_weights_for_kmeans(values))\n",
    "        mean = mean_distance(values,kmeans.cluster_centers_)\n",
    "        result[i] = mean\n",
    "        print(\"Mean for %s clusters %f \" % (str(i).zfill(3),mean))\n",
    "    return result\n",
    "        \n",
    "means_cluster_1_10 = find_clusters_number(weights,1,101,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sparsity = [(len(dict_cluster[x]),x) for x in dict_cluster]\n",
    "cluster_sparsity = sorted(cluster_sparsity, key=lambda x: x[0] )\n",
    "plt.bar([x[1] for x in cluster_sparsity], [x[0] for x in cluster_sparsity], width=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(cluster_sparsity)), [x[0] for x in sorted(cluster_sparsity, key=lambda x: x[0], reverse=True)], width=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.total_ordering\n",
    "class HuffmanNode(object):\n",
    "    def __init__(self, left=None, right=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    def children(self):\n",
    "        return((self.left, self.right))\n",
    "    def __lt__(self, other):\n",
    "        return True\n",
    "\n",
    "def create_tree(frequencies):\n",
    "    p = queue.PriorityQueue()\n",
    "    for value in frequencies:\n",
    "        p.put(value)\n",
    "    while p.qsize() > 1:\n",
    "        l, r = p.get(), p.get()\n",
    "        node = HuffmanNode(l, r)\n",
    "        p.put((l[0]+r[0], node))    \n",
    "    return p.get()\n",
    "\n",
    "# Dictionary (n : \"01010\")\n",
    "def coding_tree(node, prefix=\"\", code={}):\n",
    "    if isinstance(node[1].left[1], HuffmanNode):\n",
    "        coding_tree(node[1].left,prefix+\"0\", code)\n",
    "    else:\n",
    "        code[node[1].left[1]]=prefix+\"0\"\n",
    "    if isinstance(node[1].right[1],HuffmanNode):\n",
    "        coding_tree(node[1].right,prefix+\"1\", code)\n",
    "    else:\n",
    "        code[node[1].right[1]]=prefix+\"1\"\n",
    "    return(code)\n",
    "\n",
    "def decode(rev_huff,code):\n",
    "    for k, v in rev_huff.items():\n",
    "        if v == code:\n",
    "            return k\n",
    "        \n",
    "def encode(rev_huff,code):\n",
    "    return rev_huff[code]\n",
    "\n",
    "# Dictionary (\"01010\" : n)\n",
    "def reverse_code(huff):\n",
    "    huff_code_rev = {}    \n",
    "    for k,v in huff.items():\n",
    "        huff_code_rev[v] = k\n",
    "    return huff_code_rev\n",
    "\n",
    "def rev_encode(rev_huff,code):\n",
    "    return decode(rev_huff,code)\n",
    "        \n",
    "def rev_decode(rev_huff,code):\n",
    "    return encode(rev_huff,code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sparsity = [(len(v),k) for k,v in dict_cluster.items()]\n",
    "ht_cluster = create_tree(cluster_sparsity)\n",
    "hc_cluster = coding_tree(ht_cluster,code={})\n",
    "hc_rev = reverse_code(hc_cluster)\n",
    "\n",
    "list(hc_cluster.items())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_centroid_index_huffman(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return encode(hc_cluster,idx)\n",
    "\n",
    "def redefine_weights_huffman(weights,centers):\n",
    "    arr_ret = np.empty_like(weights).astype(str)\n",
    "    for i, row in enumerate(weights):\n",
    "        for j, col in enumerate(row):\n",
    "            arr_ret[i,j] = nearest_centroid_index_huffman(centers,weights[i,j])\n",
    "    return arr_ret\n",
    "\n",
    "def dict_index_to_cluster_huffman(weights,centers):\n",
    "        dict_ret = {}\n",
    "        for i, row in enumerate(weights):\n",
    "            for j, col in enumerate(row):\n",
    "                dict_ret[(i,j)] = nearest_centroid_index_huffman(centers,weights[i,j])\n",
    "        return dict_ret\n",
    "    \n",
    "def index_dict_to_matrix_huffman(dict_index,dict_values,huff_code,shape):\n",
    "    coord_array = np.asarray([rev_decode(huff_code,x) for x in huff_dict_index.values()])\n",
    "    return dict_values[coord_array].reshape(shape)\n",
    "\n",
    "def centroid_gradient_matrix_huffman(idx_huff_matrix,gradient,cluster):\n",
    "    return scipy.ndimage.sum(gradient,idx_huff_matrix,[x for x in cluster.values()])\n",
    "\n",
    "huff_index_matrix = redefine_weights_huffman(weights,centers)\n",
    "huff_dict_index = dict_index_to_cluster_huffman(weights,centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit index_dict_to_matrix(dict_index,centers,(300,784))\n",
    "%timeit index_dict_to_matrix_huffman(huff_dict_index,centers,hc_rev,(300,784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit centroid_gradient_matrix(matrix_index,weights,cluster_number)\n",
    "%timeit centroid_gradient_matrix_huffman(huff_index_matrix,weights,hc_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per Huffman non è possibile fare nulla (implementazione)\n",
    "# Alla fine di tutto calcolare un'ipotetica compressione (funzione che restituisce il tasso)\n",
    "# sommatoria: grandezza di ogni cluster (# elementi) * lunghezza stringa in char (8b a char)\n",
    "# in confronto con # elementi * 8b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRUNING (with kmeans support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_matrix(mat,percentage,method='out'):\n",
    "    threshold = (100-percentage)\n",
    "    \n",
    "    if method == 'inout':\n",
    "        threshold /= 4\n",
    "        perc_up,perc_down,perc_mid_up,perc_mid_down = 100 - threshold, threshold, 50 + threshold, 50 - threshold\n",
    "        percentile_up = np.percentile(mat,perc_up)\n",
    "        percentile_down = np.percentile(mat,perc_down)\n",
    "        percentile_mid_up = np.percentile(mat,perc_mid_up)\n",
    "        percentile_mid_down = np.percentile(mat,perc_mid_down)\n",
    "    else:\n",
    "        threshold /= 2\n",
    "        if method == 'in': perc_up, perc_down = 50 + threshold, 50 - threshold\n",
    "        elif method == 'out': perc_up, perc_down = 100 - threshold, threshold\n",
    "        percentile_up = np.percentile(mat,perc_up)\n",
    "        percentile_down = np.percentile(mat,perc_down)\n",
    "        \n",
    "    w_pruned = np.copy(mat)\n",
    "    for i,row in enumerate(mat):\n",
    "        for j,_ in enumerate(row):\n",
    "            if method == 'in':\n",
    "                if mat[i,j] > percentile_down and mat[i,j] < percentile_up:\n",
    "                    w_pruned[i,j] = 0\n",
    "            elif method == 'out':\n",
    "                if mat[i,j] < percentile_down or mat[i,j] > percentile_up:\n",
    "                    w_pruned[i,j] = 0\n",
    "            elif method == 'inout':\n",
    "                if mat[i,j] < percentile_down or mat[i,j] > percentile_up or (mat[i,j] > percentile_mid_down and mat[i,j] < percentile_mid_up):\n",
    "                    w_pruned[i,j] = 0\n",
    "    return w_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_csc = csc_matrix(pruning_matrix(weights,70,method='in'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_nearest_centroid_index(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    return (np.abs(centers - value)).argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_build_clusters(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(weights.data.reshape(-1,1))\n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "P_centers = P_build_clusters(256,w_csc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_redefine_weights(weights,centers):\n",
    "    new_data_idx = [nearest_centroid_index(centers,w) for w in weights.data]\n",
    "    return csc_matrix((new_data_idx,weights.indices,weights.indptr))\n",
    "\n",
    "idx_csc = P_redefine_weights(w_csc,P_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<300x784 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 164640 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def P_idx_matrix_to_matrix(idx_matrix,centers):\n",
    "    return csc_matrix((centers[idx_matrix.data].reshape(-1,),idx_matrix.indices,idx_matrix.indptr))\n",
    "\n",
    "P_idx_matrix_to_matrix(idx_csc,P_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_centroid_gradient_matrix(idx_matrix,gradient,mask,cluster):\n",
    "    gradient[mask] = 0\n",
    "    return scipy.ndimage.sum(csc_matrix(gradient).data,idx_matrix.data,index=range(cluster))\n",
    "\n",
    "mask = w_csc.A == 0\n",
    "gradient = weights.copy()\n",
    "cgm = P_centroid_gradient_matrix(idx_csc,gradient,mask,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_csc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplication\n",
    "# csr * dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(range(21)).reshape(3,7)\n",
    "b = np.array(range(20,41)).reshape(3,7)\n",
    "acc = csc_matrix(a)\n",
    "acr = csr_matrix(a)\n",
    "acc.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in place\n",
    "def sparse_sub_dense(sparse,dense,mask):\n",
    "    sparse.A -= dense\n",
    "    sparse.A[mask] = 0\n",
    "\n",
    "def sparse_sub_dense2(sparse,dense,mask):\n",
    "    b = sparse.A - dense\n",
    "    b[mask] = 0\n",
    "    return csc_matrix(b)\n",
    "\n",
    "# in place\n",
    "def sparse_sub_dense3(sparse,dense,mask):\n",
    "    sparse.data -= dense.T.reshape(1,-1)[mask.T.reshape(1,-1)]\n",
    "    \n",
    "# in place\n",
    "def sparse_sub_dense4(sparse,dense,mask):\n",
    "    sparse.data -= dense.T[mask.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (1, 0)\t3\n",
      "  (0, 1)\t2\n",
      "  (1, 2)\t1\n",
      "_________________________________\n",
      "  (1, 1)\t-6\n",
      "  (0, 2)\t-4\n",
      "_________________________________\n",
      "  (0, 0)\t-1\n",
      "  (1, 0)\t-2\n",
      "  (0, 1)\t-1\n",
      "  (1, 2)\t-6\n",
      "_________________________________\n",
      "  (0, 0)\t-1\n",
      "  (1, 0)\t-2\n",
      "  (0, 1)\t-1\n",
      "  (1, 2)\t-6\n"
     ]
    }
   ],
   "source": [
    "t = csc_matrix([[1,2,0],[3,0,1]])\n",
    "mt = t.A != 0 \n",
    "rt = np.array([[2,3,4],[5,6,7]])\n",
    "\n",
    "s1 = sparse_sub_dense(t,rt,mt)\n",
    "print(t)\n",
    "print('_________________________________')\n",
    "\n",
    "t = csc_matrix([[1,2,0],[3,0,1]])\n",
    "s2 = sparse_sub_dense2(t,rt,mt)\n",
    "print(s2)\n",
    "print('_________________________________')\n",
    "\n",
    "t = csc_matrix([[1,2,0],[3,0,1]])\n",
    "s3 = sparse_sub_dense3(t,rt,mt)\n",
    "print(t)\n",
    "print('_________________________________')\n",
    "\n",
    "t = csc_matrix([[1,2,0],[3,0,1]])\n",
    "s4 = sparse_sub_dense4(t,rt,mt)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.73 ms ± 3.42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "5.62 ms ± 19.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "1.57 ms ± 11.2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "1.16 ms ± 3.94 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "cc = w_csc.copy()\n",
    "maskBig = cc.A != 0 \n",
    "r = rand(300,784)\n",
    "%timeit sparse_sub_dense(cc,r,maskBig)\n",
    "%timeit sparse_sub_dense2(cc,r,maskBig)\n",
    "%timeit sparse_sub_dense3(cc,r,maskBig)\n",
    "%timeit sparse_sub_dense4(cc,r,maskBig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc - rand(300,784)[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_last_row_csc(matrix):\n",
    "    i = matrix.indptr[-1]\n",
    "    indptr = matrix.indptr[:-1]\n",
    "    data = matrix.data[:i]\n",
    "    indices = matrix.indices[:i]\n",
    "    return csc_matrix((data,indices,indptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1,2,1,1,2,1,1])\n",
    "np.inner(acc.A,v),acc*v.T,acc*v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [[1,2,0],[3,0,1]]\n",
    "mc = csc_matrix(m)\n",
    "ma = mc.A == 0\n",
    "mc.A -= [[1,1,1],[1,0,0]]\n",
    "mc.A[ma] = 0\n",
    "mc.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = [[1,2,0],[3,0,1]]\n",
    "mc2 = csc_matrix(m2)\n",
    "sparse_subtract_dense(mc2,[[1,1,1],[1,0,0]],ma)\n",
    "m2,mc2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "a.T.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3],[4,5,6]]).T\n",
    "m = np.array([[True,True,False],[True,False,True]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 2, 6])"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 & 0.9 & 1024 & 6.897562978386463 & 2.635 & 1.006 & 1.003 \\\\\n"
     ]
    }
   ],
   "source": [
    "def cr(p,k):\n",
    "    return (784*300)*64/((784*300*p)*math.log(k,2) + k*64)\n",
    "\n",
    "def result(p,k,t,tr,te):\n",
    "    print(1,\"&\",p,\"&\",k,\"&\",cr(p,k),\"&\",round(t/16.46,3),\"&\",round(tr/98.76,3),\"&\",round(te/97.69,3),\"\\\\\\\\\")\n",
    "\n",
    "# 60 - 256 - Epoch 004 (33m13s) Accuracy TRAIN: 99.33%\tAccuracy TEST: 98.03%\tMin: 96.13% (9)\n",
    "# 60 - 512 - Epoch 003 (25m8s)  Accuracy TRAIN: 99.37%\tAccuracy TEST: 97.97%\tMin: 95.74% (9)\n",
    "# 60 - 1024 - Epoch 005 (43m6s)  Accuracy TRAIN: 99.41%\tAccuracy TEST: 98.03%\tMin: 96.04% (9)\n",
    "\n",
    "# 75 - 256 - Epoch 006 (58m29s) Accuracy TRAIN: 99.3%\tAccuracy TEST: 97.99%\tMin: 96.43% (9)\n",
    "# 75 - 512 - Epoch 004 (46m33s) Accuracy TRAIN: 99.34%\tAccuracy TEST: 97.89%\tMin: 96.23% (9)\n",
    "# 75 - 1024 - Epoch 005 (60m21s) Accuracy TRAIN: 99.36%\tAccuracy TEST: 97.98%\tMin: 96.43% (9)\n",
    "\n",
    "# 90 - 256 - Epoch 005 (55m36s) Accuracy TRAIN: 84.34%\tAccuracy TEST: 84.42%\tMin: 63.45% (5)\n",
    "# 90 - 512 - Epoch 004 (43m30s) Accuracy TRAIN: 99.33%\tAccuracy TEST: 97.96%\tMin: 96.53% (9)\n",
    "# 90 - 1024 - Epoch 004 (43m38s) Accuracy TRAIN: 99.35%\tAccuracy TEST: 98.01%\tMin: 95.94% (9)\n",
    "\n",
    "result(0.90,1024,43.38,99.35,98.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.002709381605978, 13.795125956772926)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(784*300)*64/((784*300)*math.log(512,2) + 512*64),(784*300)*64/((784*300*0.5)*math.log(512,2) + 512*64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(256,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2222222222222223"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(784*300)/(784*300*0.45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
