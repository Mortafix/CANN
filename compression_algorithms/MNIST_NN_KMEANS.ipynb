{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK with K-MEANS for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "from random import randint\n",
    "from scipy import misc\n",
    "from scipy import special\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/mnist/'\n",
    "\n",
    "IMAGES_TRAIN = 'data_training'\n",
    "IMAGES_TEST = 'data_testing'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_CLASSES = 10\n",
    "N_FEATURES = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = DATA_PATH+IMAGES_TRAIN\n",
    "data_testing = DATA_PATH+IMAGES_TEST\n",
    "ft = gzip.open(data_training, 'rb')\n",
    "TRAINING = pickle.load(ft)\n",
    "ft.close()\n",
    "ft = gzip.open(data_testing, 'rb')\n",
    "TESTING = pickle.load(ft)\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "\n",
    "    # Stop function\n",
    "    # 0 : Fixed (stop after n epochs)\n",
    "    # 1 : Progress (stop after n epochs w/o improvements)\n",
    "    # 2 : Std Dev (stop after an improvements below n)\n",
    "    \n",
    "    def __init__(self, neurons, batchsize, stop_function, stop_parameter):\n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        \n",
    "        # Standardize random weights\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        hidden_layer = np.random.rand(self.neurons, self.input_size + 1) / self.neurons\n",
    "        output_layer = np.random.rand(self.output_size, self.neurons + 1) / self.output_size\n",
    "        self.layers = [hidden_layer, output_layer]\n",
    "        self.iteration = 0\n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "\n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu[1]):\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                self.backpropagate(input_vector, target_vector)\n",
    "            \n",
    "            # Accuracy\n",
    "            accu = self.accu(test_output)\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Messages\n",
    "            if (self.iteration == 1 or self.iteration % 10 == 0):\n",
    "                self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "                \n",
    "        # Print last epoch\n",
    "        if (self.iteration % 10 != 0):\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "\n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(layer, input_with_bias)\n",
    "            output = special.expit(output)\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target):\n",
    "        c = 1./math.sqrt(self.iteration + 10)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        self.layers[-1] -= c*np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(self.layers[-1], 200, 1).T, output_deltas)\n",
    "        self.layers[0] -= c*np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector):\n",
    "        return np.argmax(self.feed_forward(input_vector)[-1])\n",
    "\n",
    "    def accu(self, testing):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k]) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy: '+str(accu[1]).zfill(4)+'%\\tMin: '+ str(accu[0]).zfill(4)+ '% ('+str(int(accu[2]))+')'\n",
    "        print(message)\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2018-11-07 18:48:30.286673) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000\n",
      "Batch Test: 10000\n",
      "Stop Function: 3 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (37s)    Accuracy: 82.49%\tMin: 12.44% (5)\n",
      "Epoch 010 (6m10s)  Accuracy: 96.27%\tMin: 94.28% (5)\n",
      "Epoch 020 (12m37s) Accuracy: 97.16%\tMin: 95.28% (8)\n",
      "Epoch 026 (16m20s) Accuracy: 97.31%\tMin: 96.13% (9)\n",
      "\n",
      "-- Training Session End (2018-11-07 19:04:50.644967) --\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(neurons=300,batchsize=1,stop_function=1,stop_parameter=3)\n",
    "nn.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_centroid_index(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_weights = nn.getWeights()\n",
    "\n",
    "def reshape_weights_for_kmeans(weights):\n",
    "    return np.hstack(weights).reshape(-1,1)\n",
    "\n",
    "def build_clusters(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(reshape_weights_for_kmeans(weights))\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_weights(weights,centers):\n",
    "    arr_ret = np.empty_like(weights).astype(np.int16)\n",
    "    for i, row in enumerate(weights):\n",
    "        for j, col in enumerate(row):\n",
    "            arr_ret[i,j] = nearest_centroid_index(centers,weights[i,j])\n",
    "    return arr_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_gradient_matrix(idx_matrix,gradient,cluster):\n",
    "    return scipy.ndimage.sum(gradient,idx_matrix,index=range(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 256\n",
    "centers = build_clusters(cluster_number,nn_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (i,j) -> k\n",
    "def dict_index_to_cluster(weights,centers):\n",
    "        dict_ret = {}\n",
    "        for i, row in enumerate(weights):\n",
    "            for j, col in enumerate(row):\n",
    "                dict_ret[(i,j)] = nearest_centroid_index(centers,weights[i,j])\n",
    "        return dict_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_dict_to_matrix(dict_index,dict_values,shape):\n",
    "    coord_array = np.asarray(list(dict_index.values()))\n",
    "    return dict_values[coord_array].reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_index_matrix(dict_index,shape):\n",
    "    return np.asarray(list(dict_index.values())).reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_KM:\n",
    "\n",
    "    def __init__(self, neurons, batchsize, cluster, pre_weights, verbose, stop_function, stop_parameter):\n",
    "        \n",
    "        start_setting_time = dt.datetime.now()\n",
    "        \n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.verbose = verbose\n",
    "        self.cluster = cluster\n",
    "        self.iteration = 0\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        \n",
    "        # Variable for shape\n",
    "        shape_hidden = (self.neurons,self.input_size+1)\n",
    "        shape_output = (self.output_size,self.neurons+1)\n",
    "        self.layers_shape = [shape_hidden,shape_output]\n",
    "            \n",
    "        # Initialize cluster for pre-trained weights (dict with centers)\n",
    "        c_hidden = build_clusters(self.cluster,pre_weights[0])\n",
    "        c_output = build_clusters(self.cluster,pre_weights[-1])\n",
    "        self.centers = [c_hidden,c_output]\n",
    "        \n",
    "        # Initialize index matrix for pre-trained weights\n",
    "        idx_hidden = redefine_weights(pre_weights[0],c_hidden)\n",
    "        idx_output = redefine_weights(pre_weights[-1],c_output)\n",
    "        self.idx_layers = [idx_hidden,idx_output]\n",
    "        \n",
    "        # Initialize dict [map (i,j) -> k]\n",
    "        dict_hidden = dict_index_to_cluster(pre_weights[0],self.centers[0])\n",
    "        dict_output = dict_index_to_cluster(pre_weights[-1],self.centers[-1])\n",
    "        self.dict = [dict_hidden,dict_output]\n",
    "        \n",
    "        # Setting time print    \n",
    "        end_setting_time = dt.datetime.now() - start_setting_time\n",
    "        eta = divmod(end_setting_time.days * 86400 + end_setting_time.seconds, 60)\n",
    "        self.eta_print_setting = str(eta[0])+\"m\"+str(eta[1])+\"s\"\n",
    "        if self.verbose:\n",
    "            print(\"--- Setting Time: %s ---\" % self.eta_print_setting)\n",
    "    \n",
    " \n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nClusters: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,self.cluster,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "        \n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu[1]):\n",
    "            \n",
    "            # Backpropagate with feed forward\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                # from index matrix to weights matrix with centroid\n",
    "                weights = []\n",
    "                for d,c,s in zip(self.dict,self.centers,self.layers_shape):\n",
    "                    w = index_dict_to_matrix(d,c,s)\n",
    "                    weights.append(w)\n",
    "                self.backpropagate(input_vector, target_vector, weights)\n",
    "                \n",
    "            # Accuracy\n",
    "            accu = self.accu(test_output,weights)\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Messages\n",
    "            #if (self.iteration == 1 or self.iteration % 10 == 0):\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "                      \n",
    "        # Print last epoch result\n",
    "        # if self.iteration % 10 != 0:\n",
    "        #    self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "\n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector, weights):\n",
    "        outputs = []\n",
    "        for w in weights:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(w, input_with_bias)\n",
    "            output = special.expit(output) # Sigmoid function\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target, weights):\n",
    "        c = 1./math.sqrt(self.iteration + 10)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector, weights)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        gradient = np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "        cg = centroid_gradient_matrix(self.idx_layers[-1],gradient,self.cluster)\n",
    "        self.centers[-1] = self.centers[-1] - c * np.array(cg).reshape(self.cluster,1)\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(weights[-1], 300, 1).T, output_deltas)\n",
    "        gradient = np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "        cg = centroid_gradient_matrix(self.idx_layers[0],gradient,self.cluster)\n",
    "        self.centers[0] = self.centers[0] - c * np.array(cg).reshape(self.cluster,1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, input_vector, weights):\n",
    "        return self.feed_forward(input_vector,weights)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector, weights):\n",
    "        return np.argmax(self.feed_forward(input_vector,weights)[-1])\n",
    "\n",
    "    def accu(self, testing, weights):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k], weights) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy: '+str(accu[1]).zfill(4)+'%\\tMin: '+ str(accu[0]).zfill(4)+ '% ('+str(int(accu[2]))+')'\n",
    "        print(message)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers\n",
    "    \n",
    "    def minsec2sec(self,time):\n",
    "        if 'm' in time:\n",
    "            splitted = time.split('m')\n",
    "            return int(splitted[0]) * 60 + int(splitted[1][:-1])\n",
    "        else:\n",
    "            return int(time[:-1])\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_km2 = Neural_Network_KM(neurons=300,batchsize=1,cluster=256,pre_weights=nn_weights,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_km2.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sparsity = [(x,len(dict_cluster[x])) for x in dict_cluster]\n",
    "cluster_sparsity = sorted(cluster_sparsity, key=lambda x: x[0] )\n",
    "plt.bar([x[0] for x in cluster_sparsity], [x[1] for x in cluster_sparsity], width=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
