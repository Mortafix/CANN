{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK with K-MEANS for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "from random import randint\n",
    "from scipy import misc\n",
    "from scipy import special\n",
    "import scipy.ndimage\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import collections\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/mnist/'\n",
    "\n",
    "IMAGES_TRAIN = 'data_training'\n",
    "IMAGES_TEST = 'data_testing'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_CLASSES = 10\n",
    "N_FEATURES = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = DATA_PATH+IMAGES_TRAIN\n",
    "data_testing = DATA_PATH+IMAGES_TEST\n",
    "ft = gzip.open(data_training, 'rb')\n",
    "TRAINING = pickle.load(ft)\n",
    "ft.close()\n",
    "ft = gzip.open(data_testing, 'rb')\n",
    "TESTING = pickle.load(ft)\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "\n",
    "    # Stop function\n",
    "    # 0 : Fixed (stop after n epochs)\n",
    "    # 1 : Progress (stop after n epochs w/o improvements)\n",
    "    # 2 : Std Dev (stop after an improvements below n)\n",
    "    \n",
    "    def __init__(self, neurons, batchsize, stop_function, stop_parameter):\n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        self.iteration = 0\n",
    "        \n",
    "        # Standardize random weights\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        hidden_layer = np.random.rand(self.neurons, self.input_size + 1) / self.neurons\n",
    "        output_layer = np.random.rand(self.output_size, self.neurons + 1) / self.output_size\n",
    "        self.layers = [hidden_layer, output_layer]\n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "\n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu[1]):\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                self.backpropagate(input_vector, target_vector)\n",
    "            \n",
    "            # Accuracy\n",
    "            accu = self.accu(test_output)\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Messages\n",
    "            if (self.iteration == 1 or self.iteration % 10 == 0):\n",
    "                self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "                \n",
    "        # Print last epoch\n",
    "        if (self.iteration % 10 != 0):\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "\n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(layer, input_with_bias)\n",
    "            output = special.expit(output)\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target):\n",
    "        c = 1./math.sqrt(self.iteration + 10)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        self.layers[-1] -= c*np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(self.layers[-1], 200, 1).T, output_deltas)\n",
    "        self.layers[0] -= c*np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector):\n",
    "        return np.argmax(self.feed_forward(input_vector)[-1])\n",
    "\n",
    "    def accu(self, testing):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k]) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy: '+str(accu[1]).zfill(4)+'%\\tMin: '+ str(accu[0]).zfill(4)+ '% ('+str(int(accu[2]))+')'\n",
    "        print(message)\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2018-11-17 17:05:13.133558) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000\n",
      "Batch Test: 10000\n",
      "Stop Function: 3 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (39s)    Accuracy: 82.49%\tMin: 12.44% (5)\n",
      "Epoch 010 (6m30s)  Accuracy: 96.27%\tMin: 94.28% (5)\n",
      "Epoch 020 (13m2s)  Accuracy: 97.16%\tMin: 95.28% (8)\n",
      "Epoch 026 (16m59s) Accuracy: 97.31%\tMin: 96.13% (9)\n",
      "\n",
      "-- Training Session End (2018-11-17 17:22:12.733445) --\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(neurons=300,batchsize=1,stop_function=1,stop_parameter=3)\n",
    "nn.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_centroid_index(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clusters(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(np.hstack(weights).reshape(-1,1))\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix (Helper Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_weights(weights,centers):\n",
    "    arr_ret = np.empty_like(weights).astype(np.int16)\n",
    "    for i, row in enumerate(weights):\n",
    "        for j, col in enumerate(row):\n",
    "            arr_ret[i,j] = nearest_centroid_index(centers,weights[i,j])\n",
    "    return arr_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_matrix_to_matrix(idx_matrix,centers,shape):\n",
    "    return centers[idx_matrix.reshape(-1,1)].reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_gradient_matrix(idx_matrix,gradient,cluster):\n",
    "    return scipy.ndimage.sum(gradient,idx_matrix,index=range(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_KM:\n",
    "\n",
    "    def __init__(self, neurons, batchsize, cluster, pre_weights, verbose, stop_function, stop_parameter):\n",
    "        \n",
    "        start_setting_time = dt.datetime.now()\n",
    "        \n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.verbose = verbose\n",
    "        self.cluster = cluster\n",
    "        self.iteration = 0\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        \n",
    "        # Variable for shape\n",
    "        shape_hidden = (self.neurons,self.input_size+1)\n",
    "        shape_output = (self.output_size,self.neurons+1)\n",
    "        self.layers_shape = [shape_hidden,shape_output]\n",
    "            \n",
    "        # Initialize cluster for pre-trained weights (dict with centers)\n",
    "        c_hidden = build_clusters(self.cluster,pre_weights[0])\n",
    "        c_output = build_clusters(self.cluster,pre_weights[-1])\n",
    "        self.centers = [c_hidden,c_output]\n",
    "        \n",
    "        # Initialize index matrix for pre-trained weights\n",
    "        idx_hidden = redefine_weights(pre_weights[0],self.centers[0])\n",
    "        idx_output = redefine_weights(pre_weights[-1],self.centers[-1])\n",
    "        self.idx_layers = [idx_hidden,idx_output]\n",
    "        \n",
    "        # Setting time print    \n",
    "        end_setting_time = dt.datetime.now() - start_setting_time\n",
    "        eta = divmod(end_setting_time.days * 86400 + end_setting_time.seconds, 60)\n",
    "        self.eta_print_setting = str(eta[0])+\"m\"+str(eta[1])+\"s\"\n",
    "        if self.verbose:\n",
    "            print(\"--- Setting Time: %s ---\" % self.eta_print_setting)\n",
    "    \n",
    " \n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nClusters: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,self.cluster,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "        \n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu[1]):\n",
    "            \n",
    "            # Backpropagate with feed forward\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                weights = []\n",
    "                for i,c,s in zip(self.idx_layers,self.centers,self.layers_shape):\n",
    "                    w = idx_matrix_to_matrix(i,c,s)\n",
    "                    weights.append(w)\n",
    "                self.backpropagate(input_vector, target_vector, weights)\n",
    "                \n",
    "            # Accuracy\n",
    "            accu = self.accu(test_output,weights)\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Messages\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "                      \n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector, weights):\n",
    "        outputs = []\n",
    "        for w in weights:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(w, input_with_bias)\n",
    "            output = special.expit(output) # Sigmoid function\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target, weights):\n",
    "        c = 1./math.sqrt(self.iteration + 10)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector, weights)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        gradient = np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "        cg = centroid_gradient_matrix(self.idx_layers[-1],gradient,self.cluster)\n",
    "        self.centers[-1] = self.centers[-1] - c * np.array(cg).reshape(self.cluster,1)\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(weights[-1], 300, 1).T, output_deltas)\n",
    "        gradient = np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "        cg = centroid_gradient_matrix(self.idx_layers[0],gradient,self.cluster)\n",
    "        self.centers[0] = self.centers[0] - c * np.array(cg).reshape(self.cluster,1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, input_vector, weights):\n",
    "        return self.feed_forward(input_vector,weights)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector, weights):\n",
    "        return np.argmax(self.feed_forward(input_vector,weights)[-1])\n",
    "\n",
    "    def accu(self, testing, weights):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k], weights) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy: '+str(accu[1]).zfill(4)+'%\\tMin: '+ str(accu[0]).zfill(4)+ '% ('+str(int(accu[2]))+')'\n",
    "        print(message)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers\n",
    "    \n",
    "    def minsec2sec(self,time):\n",
    "        if 'm' in time:\n",
    "            splitted = time.split('m')\n",
    "            return int(splitted[0]) * 60 + int(splitted[1][:-1])\n",
    "        else:\n",
    "            return int(time[:-1])\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m2s ---\n",
      "-- Training Session Start (2018-11-17 17:22:15.887921) --\n",
      "\n",
      "Neurons: 300\n",
      "Clusters: 256\n",
      "Batch Train: 60000\n",
      "Batch Test: 10000\n",
      "Stop Function: 2 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (2m47s)  Accuracy: 96.68%\tMin: 94.09% (2)\n",
      "Epoch 002 (5m31s)  Accuracy: 96.74%\tMin: 93.12% (8)\n",
      "Epoch 003 (8m16s)  Accuracy: 96.87%\tMin: 94.35% (9)\n",
      "Epoch 004 (11m6s)  Accuracy: 96.58%\tMin: 94.85% (9)\n",
      "Epoch 005 (14m0s)  Accuracy: 97.01%\tMin: 95.04% (9)\n",
      "Epoch 006 (16m52s) Accuracy: 96.74%\tMin: 94.67% (2)\n",
      "Epoch 007 (19m45s) Accuracy: 96.81%\tMin: 94.17% (5)\n",
      "\n",
      "-- Training Session End (2018-11-17 17:42:00.901200) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_km = Neural_Network_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_matrix(mat,perc):\n",
    "    real_perc = math.floor(50 + perc / 2)\n",
    "    percentile = np.percentile(mat,real_perc)\n",
    "    w_pruned = np.copy(mat)\n",
    "    for i,row in enumerate(mat):\n",
    "        for j,_ in enumerate(row):\n",
    "            if abs(mat[i,j]) > percentile:\n",
    "                w_pruned[i,j] = 0\n",
    "    return csr_matrix(w_pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_build_clusters(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(weights.data.reshape(-1,1))\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_redefine_weights(weights,centers):\n",
    "    csr_idx = weights.copy()\n",
    "    arr_ret = np.empty_like(csr_idx.data).astype(np.int16)\n",
    "    for i,w in enumerate(csr_idx.data):\n",
    "        arr_ret[i] = nearest_centroid_index(centers,w)\n",
    "    csr_idx.data = arr_ret\n",
    "    return csr_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_idx_matrix_to_matrix(idx_matrix,centers):\n",
    "    w_csr = idx_matrix.copy()\n",
    "    w_csr.data = centers[w_csr.data]\n",
    "    return w_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P_centroid_gradient_matrix(idx_matrix,gradient,cluster):\n",
    "    return scipy.ndimage.sum(gradient,idx_matrix.todense(),index=range(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with Pruning and K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_PR_KM:\n",
    "\n",
    "    def __init__(self, neurons, batchsize, cluster, pre_weights, percentile, verbose, stop_function, stop_parameter):\n",
    "        \n",
    "        start_setting_time = dt.datetime.now()\n",
    "        \n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.percentile = percentile\n",
    "        self.verbose = verbose\n",
    "        self.cluster = cluster\n",
    "        self.iteration = 0\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        \n",
    "        # Pruning weights\n",
    "        pw_hidden = pruning_matrix(pre_weights[0],self.percentile)\n",
    "        pw_output = pruning_matrix(pre_weights[1],self.percentile)\n",
    "        self.pruned_weights = [pw_hidden,pw_output]\n",
    "        \n",
    "        # Variable for shape\n",
    "        shape_hidden = (self.neurons,self.input_size+1)\n",
    "        shape_output = (self.output_size,self.neurons+1)\n",
    "        self.layers_shape = [shape_hidden,shape_output]\n",
    "            \n",
    "        # Initialize cluster for pre-trained weights pruned\n",
    "        c_hidden = P_build_clusters(self.cluster,self.pruned_weights[0])\n",
    "        c_output = P_build_clusters(self.cluster,self.pruned_weights[-1])\n",
    "        self.centers = [c_hidden,c_output]\n",
    "        \n",
    "        # Initialize index matrix for pre-trained weights\n",
    "        idx_hidden = P_redefine_weights(self.pruned_weights[0],self.centers[0])\n",
    "        idx_output = P_redefine_weights(self.pruned_weights[-1],self.centers[-1])\n",
    "        self.idx_layers = [idx_hidden,idx_output]\n",
    "        \n",
    "        # Setting time print    \n",
    "        end_setting_time = dt.datetime.now() - start_setting_time\n",
    "        eta = divmod(end_setting_time.days * 86400 + end_setting_time.seconds, 60)\n",
    "        self.eta_print_setting = str(eta[0])+\"m\"+str(eta[1])+\"s\"\n",
    "        if self.verbose:\n",
    "            print(\"--- Setting Time: %s ---\" % self.eta_print_setting)\n",
    "    \n",
    " \n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d (%d%%)\\nBatch Test: %d (%d%%)\\nClusters: %d\\nPercentile: %d\\n%s\\n' % (self.neurons,len_batch_train,self.batchsize*100,len_batch_test,self.batchsize*100,self.cluster,self.percentile,typeTrainingPrint))\n",
    "        \n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu[1]):\n",
    "            \n",
    "            # Backpropagate with feed forward\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                weights = []\n",
    "                for i,c in zip(self.idx_layers,self.centers):\n",
    "                    w = P_idx_matrix_to_matrix(i,c)\n",
    "                    weights.append(w)\n",
    "                self.backpropagate(input_vector, target_vector, weights)\n",
    "                \n",
    "            # Accuracy\n",
    "            accu = self.accu(test_output,weights)\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Messages\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "                      \n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector, weights):\n",
    "        outputs = []\n",
    "        for w in weights:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(w.toarray(), input_with_bias)\n",
    "            output = special.expit(output) # Sigmoid function\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target, weights):\n",
    "        c = 1./math.sqrt(self.iteration + 10)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector, weights)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        gradient = np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "        cg = P_centroid_gradient_matrix(self.idx_layers[-1],gradient,self.cluster)\n",
    "        self.centers[-1] = self.centers[-1] - c * np.array(cg).reshape(self.cluster,1)\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        cleared_bias = np.delete(weights[-1].toarray(),300,1).T #np.delete(weights[-1], 300, 1).T\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(cleared_bias, output_deltas)\n",
    "        gradient = np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "        cg = P_centroid_gradient_matrix(self.idx_layers[0],gradient,self.cluster)\n",
    "        self.centers[0] = self.centers[0] - c * np.array(cg).reshape(self.cluster,1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, input_vector, weights):\n",
    "        return self.feed_forward(input_vector,weights)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector, weights):\n",
    "        return np.argmax(self.feed_forward(input_vector,weights)[-1])\n",
    "\n",
    "    def accu(self, testing, weights):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k], weights) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy: '+str(accu[1]).zfill(4)+'%\\tMin: '+ str(accu[0]).zfill(4)+ '% ('+str(int(accu[2]))+')'\n",
    "        print(message)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers\n",
    "    \n",
    "    def minsec2sec(self,time):\n",
    "        if 'm' in time:\n",
    "            splitted = time.split('m')\n",
    "            return int(splitted[0]) * 60 + int(splitted[1][:-1])\n",
    "        else:\n",
    "            return int(time[:-1])\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m1s ---\n",
      "-- Training Session Start (2018-11-18 12:07:35.802561) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Clusters: 256\n",
      "Percentile: 60\n",
      "Stop Function: 2 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (3m49s)  Accuracy: 72.56%\tMin: 30.49% (5)\n",
      "Epoch 002 (7m36s)  Accuracy: 76.78%\tMin: 49.1% (5)\n",
      "Epoch 003 (11m23s) Accuracy: 79.47%\tMin: 54.37% (5)\n",
      "Epoch 004 (15m9s)  Accuracy: 79.27%\tMin: 53.81% (5)\n",
      "Epoch 005 (18m55s) Accuracy: 80.15%\tMin: 57.4% (5)\n",
      "Epoch 006 (22m41s) Accuracy: 80.14%\tMin: 57.29% (5)\n",
      "Epoch 007 (26m27s) Accuracy: 80.74%\tMin: 64.13% (5)\n",
      "Epoch 008 (30m14s) Accuracy: 81.05%\tMin: 59.53% (5)\n",
      "Epoch 009 (33m59s) Accuracy: 81.46%\tMin: 64.91% (5)\n",
      "Epoch 010 (37m46s) Accuracy: 81.22%\tMin: 60.43% (5)\n",
      "Epoch 011 (41m32s) Accuracy: 81.19%\tMin: 59.42% (5)\n",
      "\n",
      "-- Training Session End (2018-11-18 12:49:07.812451) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,percentile=60,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m1s ---\n",
      "-- Training Session Start (2018-11-18 12:49:09.512794) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Clusters: 256\n",
      "Percentile: 75\n",
      "Stop Function: 2 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (3m26s)  Accuracy: 10.28%\tMin: 00.0% (0)\n",
      "Epoch 002 (6m52s)  Accuracy: 10.28%\tMin: 00.0% (0)\n",
      "Epoch 003 (10m19s) Accuracy: 57.58%\tMin: 00.0% (9)\n",
      "Epoch 004 (13m45s) Accuracy: 59.07%\tMin: 00.1% (9)\n",
      "Epoch 005 (17m12s) Accuracy: 58.92%\tMin: 00.0% (6)\n",
      "Epoch 006 (20m38s) Accuracy: 58.91%\tMin: 00.0% (6)\n",
      "\n",
      "-- Training Session End (2018-11-18 13:09:48.140424) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,percentile=75,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m7s ---\n",
      "-- Training Session Start (2018-11-18 13:09:55.967590) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Clusters: 256\n",
      "Percentile: 80\n",
      "Stop Function: 2 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (3m50s)  Accuracy: 18.67%\tMin: 00.0% (0)\n",
      "Epoch 002 (7m40s)  Accuracy: 18.25%\tMin: 00.0% (3)\n",
      "Epoch 003 (11m30s) Accuracy: 18.25%\tMin: 00.0% (3)\n",
      "\n",
      "-- Training Session End (2018-11-18 13:21:26.526439) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,percentile=80,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m2s ---\n",
      "-- Training Session Start (2018-11-18 13:21:28.587445) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Clusters: 256\n",
      "Percentile: 90\n",
      "Stop Function: 2 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (3m38s)  Accuracy: 54.54%\tMin: 00.0% (9)\n",
      "Epoch 002 (7m16s)  Accuracy: 62.56%\tMin: 00.0% (8)\n",
      "Epoch 003 (10m54s) Accuracy: 66.16%\tMin: 00.0% (8)\n",
      "Epoch 004 (14m33s) Accuracy: 63.86%\tMin: 00.0% (8)\n",
      "Epoch 005 (18m11s) Accuracy: 66.99%\tMin: 00.0% (8)\n",
      "Epoch 006 (21m50s) Accuracy: 66.7%\tMin: 00.0% (8)\n",
      "Epoch 007 (25m28s) Accuracy: 63.12%\tMin: 00.0% (8)\n",
      "\n",
      "-- Training Session End (2018-11-18 13:46:57.458809) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,percentile=90,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m1s ---\n",
      "-- Training Session Start (2018-11-18 13:46:59.461193) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Clusters: 256\n",
      "Percentile: 94\n",
      "Stop Function: 2 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (3m46s)  Accuracy: 51.31%\tMin: 00.0% (3)\n",
      "Epoch 002 (7m32s)  Accuracy: 51.55%\tMin: 00.0% (8)\n",
      "Epoch 003 (11m19s) Accuracy: 55.81%\tMin: 00.0% (8)\n",
      "Epoch 004 (15m6s)  Accuracy: 58.34%\tMin: 00.0% (8)\n",
      "Epoch 005 (18m53s) Accuracy: 58.25%\tMin: 00.0% (8)\n",
      "Epoch 006 (22m40s) Accuracy: 58.2%\tMin: 00.0% (8)\n",
      "\n",
      "-- Training Session End (2018-11-18 14:09:39.665675) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,percentile=94,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m2s ---\n",
      "-- Training Session Start (2018-11-18 14:09:41.846824) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Clusters: 256\n",
      "Percentile: 98\n",
      "Stop Function: 2 epoch(s) w/o improvements\n",
      "\n",
      "Epoch 001 (3m39s)  Accuracy: 64.02%\tMin: 00.0% (8)\n",
      "Epoch 002 (7m19s)  Accuracy: 65.91%\tMin: 00.0% (8)\n",
      "Epoch 003 (10m58s) Accuracy: 66.38%\tMin: 00.0% (8)\n",
      "Epoch 004 (14m37s) Accuracy: 66.8%\tMin: 00.0% (8)\n",
      "Epoch 005 (18m17s) Accuracy: 66.25%\tMin: 00.0% (8)\n",
      "Epoch 006 (21m57s) Accuracy: 67.14%\tMin: 00.0% (8)\n",
      "Epoch 007 (25m37s) Accuracy: 67.34%\tMin: 00.0% (8)\n",
      "Epoch 008 (29m16s) Accuracy: 67.98%\tMin: 00.0% (8)\n",
      "Epoch 009 (32m56s) Accuracy: 67.84%\tMin: 00.0% (8)\n",
      "Epoch 010 (36m36s) Accuracy: 67.7%\tMin: 00.0% (8)\n",
      "\n",
      "-- Training Session End (2018-11-18 14:46:17.916968) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,percentile=98,verbose=True,stop_function=1,stop_parameter=2)\n",
    "nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
