{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "from random import randint\n",
    "from scipy import misc\n",
    "from scipy import special\n",
    "import scipy.ndimage\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import csv\n",
    "import collections\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/mnist/'\n",
    "\n",
    "IMAGES_TRAIN = 'data_training'\n",
    "IMAGES_TEST = 'data_testing'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_CLASSES = 10\n",
    "N_FEATURES = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = DATA_PATH+IMAGES_TRAIN\n",
    "data_testing = DATA_PATH+IMAGES_TEST\n",
    "ft = gzip.open(data_training, 'rb')\n",
    "TRAINING = pickle.load(ft)\n",
    "ft.close()\n",
    "ft = gzip.open(data_testing, 'rb')\n",
    "TESTING = pickle.load(ft)\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "    \n",
    "    def __init__(self, neurons, batchsize, stop_function, stop_parameter):\n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        self.iteration = 0\n",
    "        \n",
    "        # Standardize random weights\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        hidden_layer = np.random.rand(self.neurons, self.input_size + 1) / self.neurons\n",
    "        output_layer = np.random.rand(self.output_size, self.neurons + 1) / self.output_size\n",
    "        self.layers = [hidden_layer, output_layer]\n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu_train = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        test_input = training[0:len_batch_train][0:len_batch_train]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "\n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu_train[1]):\n",
    "            \n",
    "            self.iteration += 1\n",
    "            \n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                self.backpropagate(input_vector, target_vector)\n",
    "            \n",
    "            # Accuracy\n",
    "            accu_test = self.accu(test_output)\n",
    "            accu_train = self.accu(test_input)\n",
    "            \n",
    "            # Messages\n",
    "            if (self.iteration == 1 or self.iteration % 10 == 0):\n",
    "                self.print_message_iter(self.iteration,accu_test,accu_train,self.ETAepoch(self.start_time))\n",
    "                \n",
    "        # Print last epoch\n",
    "        if (self.iteration % 10 != 0):\n",
    "            self.print_message_iter(self.iteration,accu_test,accu_train,self.ETAepoch(self.start_time))\n",
    "\n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(layer, input_with_bias)\n",
    "            output = special.expit(output)\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target):\n",
    "        c = 10**(-4) + 10**(-1)/math.sqrt(self.iteration)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        self.layers[-1] -= c*np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(self.layers[-1], self.neurons, 1).T, output_deltas)\n",
    "        self.layers[0] -= c*np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector):\n",
    "        return np.argmax(self.feed_forward(input_vector)[-1])\n",
    "\n",
    "    def accu(self, testing_batch):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing_batch[1])):\n",
    "            if self.predict_one(testing_batch[0][k]) == testing_batch[1][k]:\n",
    "                res[testing_batch[1][k]] += 1\n",
    "            else:\n",
    "                res[testing_batch[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu_test,accu_train,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy TRAIN: '+str(accu_train[1]).zfill(4)+'%\\t'\n",
    "        message += 'Accuracy TEST: '+str(accu_test[1]).zfill(4)+'%\\t'\n",
    "        message += 'Min: '+ str(accu_test[0]).zfill(4)+ '% ('+str(int(accu_test[2]))+')'\n",
    "        print(message)\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-22 16:31:45.401925) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000\n",
      "Batch Test: 10000\n",
      "Stop Function: improvements below 0.01%\n",
      "\n",
      "Epoch 001 (42s)    Accuracy TRAIN: 91.11%\tAccuracy TEST: 91.53%\tMin: 78.59% (5)\n",
      "Epoch 010 (6m58s)  Accuracy TRAIN: 97.74%\tAccuracy TEST: 97.0%\tMin: 94.85% (9)\n",
      "Epoch 020 (14m0s)  Accuracy TRAIN: 98.6%\tAccuracy TEST: 97.63%\tMin: 96.3% (7)\n",
      "Epoch 030 (21m8s)  Accuracy TRAIN: 98.93%\tAccuracy TEST: 97.78%\tMin: 96.63% (9)\n",
      "Epoch 033 (23m17s) Accuracy TRAIN: 98.99%\tAccuracy TEST: 97.8%\tMin: 96.63% (9)\n",
      "\n",
      "-- Training Session End (2019-01-22 16:55:02.655120) --\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(neurons=300,batchsize=1,stop_function=2,stop_parameter=0.01)\n",
    "nn.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK with CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_centroid_index(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clusters(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(np.hstack(weights).reshape(-1,1))\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix - Helping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_weights(weights,centers):\n",
    "    arr_ret = np.empty_like(weights).astype(np.int16)\n",
    "    for i, row in enumerate(weights):\n",
    "        for j, col in enumerate(row):\n",
    "            arr_ret[i,j] = nearest_centroid_index(centers,weights[i,j])\n",
    "    return arr_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_matrix_to_matrix(idx_matrix,centers,shape):\n",
    "    return centers[idx_matrix.reshape(-1,1)].reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_gradient_matrix(idx_matrix,gradient,cluster):\n",
    "    return scipy.ndimage.sum(gradient,idx_matrix,index=range(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_KM:\n",
    "\n",
    "    def __init__(self, neurons, batchsize, cluster, pre_weights, stop_function, stop_parameter):\n",
    "        \n",
    "        start_setting_time = dt.datetime.now()\n",
    "        \n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.cluster = cluster\n",
    "        self.iteration = 0\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        \n",
    "        # Variable for shape\n",
    "        shape_hidden = (self.neurons,self.input_size+1)\n",
    "        shape_output = (self.output_size,self.neurons+1)\n",
    "        self.layers_shape = [shape_hidden,shape_output]\n",
    "            \n",
    "        # Initialize cluster for pre-trained weights (dict with centers)\n",
    "        c_hidden = build_clusters(self.cluster,pre_weights[0])\n",
    "        c_output = build_clusters(self.cluster,pre_weights[-1])\n",
    "        self.centers = [c_hidden,c_output]\n",
    "        \n",
    "        # Initialize index matrix for pre-trained weights\n",
    "        idx_hidden = redefine_weights(pre_weights[0],self.centers[0])\n",
    "        idx_output = redefine_weights(pre_weights[-1],self.centers[-1])\n",
    "        self.idx_layers = [idx_hidden,idx_output]\n",
    "        \n",
    "        # Setting time print    \n",
    "        end_setting_time = dt.datetime.now() - start_setting_time\n",
    "        eta = divmod(end_setting_time.days * 86400 + end_setting_time.seconds, 60)\n",
    "        self.eta_print_setting = str(eta[0])+\"m\"+str(eta[1])+\"s\"\n",
    "    \n",
    " \n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu_train = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        test_input = training[0:len_batch_train][0:len_batch_train]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nClusters: %d\\nBatch Train: %d (%d%%)\\nBatch Test: %d (%d%%)\\n%s\\n' % (self.neurons,self.cluster,len_batch_train,self.batchsize*100,len_batch_test,self.batchsize*100,typeTrainingPrint))\n",
    "        \n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu_train[1]):\n",
    "            \n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Backpropagate with feed forward\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                weights = []\n",
    "                for i,c,s in zip(self.idx_layers,self.centers,self.layers_shape):\n",
    "                    w = idx_matrix_to_matrix(i,c,s)\n",
    "                    weights.append(w)\n",
    "                self.backpropagate(input_vector, target_vector, weights)\n",
    "                \n",
    "            # Accuracy\n",
    "            accu_test = self.accu(test_output,weights)\n",
    "            accu_train = self.accu(test_input,weights)\n",
    "            \n",
    "            # Messages\n",
    "            self.print_message_iter(self.iteration,accu_test,accu_train,self.ETAepoch(self.start_time))\n",
    "                      \n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "        print('-------------------------')\n",
    "        print(self.printLineCSV(self.cluster,self.ETAepoch(self.start_time),accu_train,accu_test))\n",
    "        print('-------------------------\\n')\n",
    "\n",
    "    def feed_forward(self, input_vector, weights):\n",
    "        outputs = []\n",
    "        for w in weights:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(w, input_with_bias)\n",
    "            output = special.expit(output) # Sigmoid function\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target, weights):\n",
    "        c = 10**(-4) + (10**(-1))/math.sqrt(self.iteration)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector, weights)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        gradient = np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "        cg = centroid_gradient_matrix(self.idx_layers[-1],gradient,self.cluster)\n",
    "        self.centers[-1] = self.centers[-1] - c * np.array(cg).reshape(self.cluster,1)\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(weights[-1], self.neurons, 1).T, output_deltas)\n",
    "        gradient = np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "        cg = centroid_gradient_matrix(self.idx_layers[0],gradient,self.cluster)\n",
    "        self.centers[0] = self.centers[0] - c * np.array(cg).reshape(self.cluster,1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, input_vector, weights):\n",
    "        return self.feed_forward(input_vector,weights)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector, weights):\n",
    "        return np.argmax(self.feed_forward(input_vector,weights)[-1])\n",
    "\n",
    "    def accu(self, testing, weights):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k], weights) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu_test,accu_train,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy TRAIN: '+str(accu_train[1]).zfill(4)+'%\\t'\n",
    "        message += 'Accuracy TEST: '+str(accu_test[1]).zfill(4)+'%\\t'\n",
    "        message += 'Min: '+ str(accu_test[0]).zfill(4)+ '% ('+str(int(accu_test[2]))+')'\n",
    "        print(message)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers\n",
    "    \n",
    "    def minsec2sec(self,time):\n",
    "        if 'm' in time:\n",
    "            splitted = time.split('m')\n",
    "            return int(splitted[0]) * 60 + int(splitted[1][:-1])\n",
    "        else:\n",
    "            return int(time[:-1])\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "    \n",
    "    def printLineCSV(self,cluster,time,a_train,a_test):\n",
    "        cr = round((784*300)*64/((784*300)*math.log(cluster,2) + cluster*64),3)\n",
    "        return str(cluster)+','+str(cr)+','+time+','+str(a_train[1])+','+str(a_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-22 17:07:57.714760) --\n",
      "\n",
      "Neurons: 300\n",
      "Clusters: 256\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.01%\n",
      "\n",
      "Epoch 001 (3m0s)   Accuracy TRAIN: 98.92%\tAccuracy TEST: 97.5%\tMin: 95.24% (9)\n",
      "Epoch 002 (6m0s)   Accuracy TRAIN: 98.93%\tAccuracy TEST: 97.48%\tMin: 96.23% (9)\n",
      "\n",
      "-- Training Session End (2019-01-22 17:13:58.625813) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_km = Neural_Network_KM(neurons=300,batchsize=1,cluster=256,pre_weights=pre_trained_weights,stop_function=2,stop_parameter=0.01)\n",
    "nn_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK with PRUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC - Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning_matrix(mat,percentage,method='out'):\n",
    "    threshold = (100-percentage)\n",
    "    \n",
    "    if method == 'inout':\n",
    "        threshold /= 4\n",
    "        perc_up,perc_down,perc_mid_up,perc_mid_down = 100 - threshold, threshold, 50 + threshold, 50 - threshold\n",
    "        percentile_up = np.percentile(mat,perc_up)\n",
    "        percentile_down = np.percentile(mat,perc_down)\n",
    "        percentile_mid_up = np.percentile(mat,perc_mid_up)\n",
    "        percentile_mid_down = np.percentile(mat,perc_mid_down)\n",
    "    else:\n",
    "        threshold /= 2\n",
    "        if method == 'in': perc_up, perc_down = 50 + threshold, 50 - threshold\n",
    "        elif method == 'out': perc_up, perc_down = 100 - threshold, threshold\n",
    "        percentile_up = np.percentile(mat,perc_up)\n",
    "        percentile_down = np.percentile(mat,perc_down)\n",
    "        \n",
    "    w_pruned = np.copy(mat)\n",
    "    for i,row in enumerate(mat):\n",
    "        for j,_ in enumerate(row):\n",
    "            if method == 'in':\n",
    "                if mat[i,j] > percentile_down and mat[i,j] < percentile_up:\n",
    "                    w_pruned[i,j] = 0\n",
    "            elif method == 'out':\n",
    "                if mat[i,j] < percentile_down or mat[i,j] > percentile_up:\n",
    "                    w_pruned[i,j] = 0\n",
    "            elif method == 'inout':\n",
    "                if mat[i,j] < percentile_down or mat[i,j] > percentile_up or (mat[i,j] > percentile_mid_down and mat[i,j] < percentile_mid_up):\n",
    "                    w_pruned[i,j] = 0\n",
    "    return w_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# in place\n",
    "def sparse_sub_dense(sparse,dense,mask):\n",
    "    b = sparse - dense\n",
    "    b[mask] = 0\n",
    "    return csc_matrix(b)''' # old\n",
    "\n",
    "# in place\n",
    "def sparse_sub_dense(sparse,dense,mask):\n",
    "    sparse.data -= dense.T[mask.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_last_row(csc):\n",
    "    i = csc.indptr[-1]\n",
    "    indptr = csc.indptr[:-1]\n",
    "    data = csc.data[:i]\n",
    "    indices = csc.indices[:i]\n",
    "    return csc_matrix((data,indices,indptr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_PR_CSC:\n",
    "    \n",
    "    def __init__(self, neurons, batchsize, stop_function, stop_parameter, weights=None, pruning=None, pruning_method=None):\n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.pre_trained_weights = weights\n",
    "        self.pruning = pruning\n",
    "        self.pruning_method = pruning_method\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        self.iteration = 0\n",
    "        \n",
    "        if weights == None:\n",
    "            # Standardize random weights\n",
    "            np.random.seed(RANDOM_SEED)\n",
    "            hidden_layer = np.random.rand(self.neurons, self.input_size + 1) / self.neurons\n",
    "            output_layer = np.random.rand(self.output_size, self.neurons + 1) / self.output_size\n",
    "            self.layers = [hidden_layer, output_layer]\n",
    "        else:\n",
    "            # Pruning weights\n",
    "            pw_hidden = csc_matrix(pruning_matrix(pre_trained_weights[0],self.pruning,self.pruning_method))\n",
    "            pw_output = csc_matrix(pruning_matrix(pre_trained_weights[1],self.pruning,self.pruning_method))\n",
    "            self.layers = [pw_hidden, pw_output]\n",
    "            # Matrix mask\n",
    "            mask_hidden = pw_hidden.A != 0 \n",
    "            mask_output = pw_output.A != 0\n",
    "            self.masks = [mask_hidden,mask_output]\n",
    "\n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu_train = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d (%d%%)\\nBatch Test: %d (%d%%)\\nPruning: %d%% (%s)\\n%s\\n' % (self.neurons,len_batch_train,self.batchsize*100,len_batch_test,self.batchsize*100,self.pruning,self.pruning_method,typeTrainingPrint))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        test_input = training[0:len_batch_train][0:len_batch_train]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "\n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu_train[1]):\n",
    "            \n",
    "            self.iteration += 1\n",
    "            \n",
    "            #print(self.layers[0])\n",
    "            \n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                self.backpropagate(input_vector, target_vector)\n",
    "            \n",
    "            # Accuracy\n",
    "            accu_test = self.accu(test_output)\n",
    "            accu_train = self.accu(test_input)\n",
    "            \n",
    "            # Messages\n",
    "            self.print_message_iter(self.iteration,accu_test,accu_train,self.ETAepoch(self.start_time))\n",
    "    \n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "        print('-------------------------')\n",
    "        print(self.printLineCSV(self.pruning,self.ETAepoch(self.start_time),accu_train,accu_test))\n",
    "        print('-------------------------\\n')\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = layer * input_with_bias\n",
    "            output = special.expit(output)\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target):\n",
    "        c = 10**(-4) + (10**(-1))/math.sqrt(self.iteration)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector)\n",
    "        \n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        sparse_sub_dense(self.layers[-1],c*np.outer(output_deltas, np.append(hidden_outputs, 1)),self.masks[-1])\n",
    "        \n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * (delete_last_row(self.layers[-1]).T * output_deltas)\n",
    "        sparse_sub_dense(self.layers[0],c*np.outer(hidden_deltas, np.append(input_vector, 1)),self.masks[0])\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector):\n",
    "        return np.argmax(self.feed_forward(input_vector)[-1])\n",
    "\n",
    "    def accu(self, testing_batch):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing_batch[1])):\n",
    "            if self.predict_one(testing_batch[0][k]) == testing_batch[1][k]:\n",
    "                res[testing_batch[1][k]] += 1\n",
    "            else:\n",
    "                res[testing_batch[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu_test,accu_train,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy TRAIN: '+str(accu_train[1]).zfill(4)+'%\\t'\n",
    "        message += 'Accuracy TEST: '+str(accu_test[1]).zfill(4)+'%\\t'\n",
    "        message += 'Min: '+ str(accu_test[0]).zfill(4)+ '% ('+str(int(accu_test[2]))+')'\n",
    "        print(message)\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers\n",
    "    \n",
    "    def printLineCSV(self,pruning,time,a_train,a_test):\n",
    "        pr = pruning/100\n",
    "        cr = round((784*300)/(784*300*pr),3)\n",
    "        return str(pr)+','+str(cr)+','+time+','+str(a_train[1])+','+str(a_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-22 17:15:34.146234) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Pruning: 60% (in)\n",
      "Stop Function: improvements below 0.01%\n",
      "\n",
      "Epoch 001 (1m54s)  Accuracy TRAIN: 98.82%\tAccuracy TEST: 97.56%\tMin: 94.94% (7)\n",
      "Epoch 002 (3m54s)  Accuracy TRAIN: 99.03%\tAccuracy TEST: 97.78%\tMin: 96.69% (7)\n",
      "Epoch 003 (5m49s)  Accuracy TRAIN: 99.11%\tAccuracy TEST: 97.87%\tMin: 96.89% (7)\n",
      "Epoch 004 (7m43s)  Accuracy TRAIN: 99.17%\tAccuracy TEST: 97.92%\tMin: 97.08% (7)\n",
      "Epoch 005 (9m38s)  Accuracy TRAIN: 99.21%\tAccuracy TEST: 97.98%\tMin: 97.13% (9)\n",
      "Epoch 006 (11m32s) Accuracy TRAIN: 99.25%\tAccuracy TEST: 98.0%\tMin: 96.93% (9)\n",
      "Epoch 007 (13m29s) Accuracy TRAIN: 99.27%\tAccuracy TEST: 98.02%\tMin: 96.83% (9)\n",
      "Epoch 008 (15m23s) Accuracy TRAIN: 99.3%\tAccuracy TEST: 98.0%\tMin: 96.73% (9)\n",
      "Epoch 009 (17m18s) Accuracy TRAIN: 99.32%\tAccuracy TEST: 98.0%\tMin: 96.63% (9)\n",
      "Epoch 010 (19m16s) Accuracy TRAIN: 99.35%\tAccuracy TEST: 97.98%\tMin: 96.43% (9)\n",
      "Epoch 011 (21m16s) Accuracy TRAIN: 99.37%\tAccuracy TEST: 98.03%\tMin: 96.53% (9)\n",
      "Epoch 012 (23m12s) Accuracy TRAIN: 99.38%\tAccuracy TEST: 98.03%\tMin: 96.53% (9)\n",
      "\n",
      "-- Training Session End (2019-01-22 17:38:46.927640) --\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_csc = Neural_Network_PR_CSC(neurons=300,batchsize=1,weights=pre_trained_weights,pruning=60,pruning_method='in',stop_function=2,stop_parameter=0.01)\n",
    "nn_pr_csc.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK with PRUNING and CLUSTERING "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_centroid_index(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clusters_pruning(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(weights.data.reshape(-1,1))\n",
    "    return kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_weights_pruning(weights,centers):\n",
    "    new_data_idx = [nearest_centroid_index(centers,w) for w in weights.data]\n",
    "    return csc_matrix((new_data_idx,weights.indices,weights.indptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_matrix_to_matrix_pruning(idx_matrix,centers):\n",
    "    return csc_matrix((centers[idx_matrix.data].reshape(-1,),idx_matrix.indices,idx_matrix.indptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_gradient_pruning(idx_matrix,gradient,mask,cluster):\n",
    "    gradient += 0.000000001\n",
    "    gradient[mask] = 0\n",
    "    return scipy.ndimage.sum(csc_matrix(gradient).data,idx_matrix.data,index=range(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_PR_KM:\n",
    "\n",
    "    def __init__(self, neurons, batchsize, cluster, pruning, pre_weights, stop_function, stop_parameter):\n",
    "        \n",
    "        start_setting_time = dt.datetime.now()\n",
    "        \n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.pruning = pruning\n",
    "        self.cluster = cluster\n",
    "        self.iteration = 0\n",
    "        self.stop_f = stop_function\n",
    "        self.stop_p = stop_parameter\n",
    "        self.best = 0.\n",
    "        self.same = 0\n",
    "        \n",
    "        # Matrix mask\n",
    "        mask_hidden = pre_weights[0].A == 0 \n",
    "        mask_output = pre_weights[-1].A == 0\n",
    "        self.masks = [mask_hidden,mask_output]\n",
    "        \n",
    "        # Variable for shape\n",
    "        shape_hidden = (self.neurons,self.input_size+1)\n",
    "        shape_output = (self.output_size,self.neurons+1)\n",
    "        self.layers_shape = [shape_hidden,shape_output]\n",
    "            \n",
    "        # Initialize cluster for pre-trained weights (dict with centers)\n",
    "        c_hidden = build_clusters_pruning(self.cluster,pre_weights[0])\n",
    "        c_output = build_clusters_pruning(self.cluster,pre_weights[-1])\n",
    "        self.centers = [c_hidden,c_output]\n",
    "        \n",
    "        # Initialize index matrix for pre-trained weights\n",
    "        idx_hidden = redefine_weights_pruning(pre_weights[0],self.centers[0])\n",
    "        idx_output = redefine_weights_pruning(pre_weights[-1],self.centers[-1])\n",
    "        self.idx_layers = [idx_hidden,idx_output]\n",
    "        \n",
    "        # Setting time print    \n",
    "        end_setting_time = dt.datetime.now() - start_setting_time\n",
    "        eta = divmod(end_setting_time.days * 86400 + end_setting_time.seconds, 60)\n",
    "        self.eta_print_setting = str(eta[0])+\"m\"+str(eta[1])+\"s\"\n",
    "    \n",
    " \n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        accu_train = [0.,0.]\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        test_input = training[0:len_batch_train][0:len_batch_train]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Stop Function: \"    \n",
    "        if self.stop_f == 0:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epochs\"\n",
    "        elif self.stop_f == 1:\n",
    "            typeTrainingPrint += str(self.stop_p)+\" epoch(s) w/o improvements\"\n",
    "        elif self.stop_f == 2:\n",
    "            typeTrainingPrint += \"improvements below \"+str(self.stop_p)+\"%\"\n",
    "        print('\\nNeurons: %d\\nPruning: %.2f\\nClusters: %d\\nBatch Train: %d (%d%%)\\nBatch Test: %d (%d%%)\\n%s\\n' % (self.neurons,self.pruning/100,self.cluster,len_batch_train,self.batchsize*100,len_batch_test,self.batchsize*100,typeTrainingPrint))\n",
    "        \n",
    "        # Performs iterations\n",
    "        while not self.is_stop_function_enabled(accu_train[1]):\n",
    "            \n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Backpropagate with feed forward\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                weights = []\n",
    "                for i,c in zip(self.idx_layers,self.centers):\n",
    "                    w = idx_matrix_to_matrix_pruning(i,c)\n",
    "                    weights.append(w)\n",
    "                self.backpropagate(input_vector, target_vector, weights)\n",
    "                \n",
    "            # Accuracy\n",
    "            accu_test = self.accu(test_output,weights)\n",
    "            accu_train = self.accu(test_input,weights)\n",
    "            \n",
    "            # Messages\n",
    "            self.print_message_iter(self.iteration,accu_test,accu_train,self.ETAepoch(self.start_time))\n",
    "                      \n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "        print('-------------------------')\n",
    "        print(self.printLineCSV(self.pruning,self.cluster,self.ETAepoch(self.start_time),accu_train,accu_test))\n",
    "        print('-------------------------\\n')\n",
    "\n",
    "    def feed_forward(self, input_vector, weights):\n",
    "        outputs = []\n",
    "        for w in weights:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = w * input_with_bias\n",
    "            output = special.expit(output)\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs                            \n",
    "                                  \n",
    "    def backpropagate(self, input_vector, target, weights):\n",
    "        c = 10**(-4) + (10**(-1))/math.sqrt(self.iteration)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector, weights)\n",
    "        \n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        gradient = np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "        cg = centroid_gradient_pruning(self.idx_layers[-1],gradient,self.masks[-1],self.cluster)\n",
    "        self.centers[-1] = self.centers[-1] - (c * np.array(cg)).reshape(-1,1)\n",
    "        \n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * (delete_last_row(weights[-1]).T * output_deltas)\n",
    "        gradient = np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "        cg = centroid_gradient_pruning(self.idx_layers[0],gradient,self.masks[0],self.cluster)\n",
    "        self.centers[0] = self.centers[0] - (c * np.array(cg)).reshape(-1,1)\n",
    "        \n",
    "    \n",
    "    def predict(self, input_vector, weights):\n",
    "        return self.feed_forward(input_vector,weights)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector, weights):\n",
    "        return np.argmax(self.feed_forward(input_vector,weights)[-1])\n",
    "\n",
    "    def accu(self, testing, weights):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k], weights) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    \n",
    "    def is_stop_function_enabled(self,accuracy):\n",
    "        if self.stop_f == 0:\n",
    "            if self.iteration < self.stop_p:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif self.stop_f == 1:\n",
    "            if accuracy > self.best or self.iteration == 0:\n",
    "                self.same = 0\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                self.same += 1\n",
    "                if self.same < self.stop_p:\n",
    "                    return False\n",
    "                else:\n",
    "                    return True\n",
    "        elif self.stop_f == 2:\n",
    "            if accuracy > self.best + self.stop_p or self.iteration == 0:\n",
    "                self.best = accuracy\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu_test,accu_train,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Accuracy TRAIN: '+str(accu_train[1]).zfill(4)+'%\\t'\n",
    "        message += 'Accuracy TEST: '+str(accu_test[1]).zfill(4)+'%\\t'\n",
    "        message += 'Min: '+ str(accu_test[0]).zfill(4)+ '% ('+str(int(accu_test[2]))+')'\n",
    "        print(message)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.idx_layers,self.centers\n",
    "    \n",
    "    def minsec2sec(self,time):\n",
    "        if 'm' in time:\n",
    "            splitted = time.split('m')\n",
    "            return int(splitted[0]) * 60 + int(splitted[1][:-1])\n",
    "        else:\n",
    "            return int(time[:-1])\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "    \n",
    "    def printLineCSV(self,pruning,cluster,time,a_train,a_test):\n",
    "        pr = pruning/100\n",
    "        cr = round((784*300)*64/((784*300*pr)*math.log(cluster,2) + cluster*64),3)\n",
    "        return str(pr)+','+str(cluster)+','+str(cr)+','+time+','+str(a_train[1])+','+str(a_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Moris/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning: init_size=300 should be larger than k=512. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-22 18:20:42.356691) --\n",
      "\n",
      "Neurons: 300\n",
      "Pruning: 0.60\n",
      "Clusters: 512\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.01%\n",
      "\n",
      "Epoch 001 (8m54s)  Accuracy TRAIN: 99.04%\tAccuracy TEST: 97.77%\tMin: 96.04% (9)\n",
      "Epoch 002 (17m47s) Accuracy TRAIN: 99.07%\tAccuracy TEST: 97.73%\tMin: 95.34% (9)\n",
      "Epoch 003 (26m34s) Accuracy TRAIN: 99.08%\tAccuracy TEST: 97.74%\tMin: 95.64% (9)\n",
      "\n",
      "-- Training Session End (2019-01-22 18:47:16.776786) --\n",
      "-------------------------\n",
      "0.6,512,11.554,26m34s,97.74,99.08\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pruned_weights = nn_pr_csc.getWeights()\n",
    "nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=512,pruning=60,pre_weights=pruned_weights,stop_function=2,stop_parameter=0.01)\n",
    "nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruining e Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(70,100,10):\n",
    "    pre_trained_weights = nn.getWeights()\n",
    "    nn_pr_csc = Neural_Network_PR_CSC(neurons=300,batchsize=1,weights=pre_trained_weights,pruning=i,pruning_method='in',stop_function=2,stop_parameter=0.02)\n",
    "    nn_pr_csc.train(TRAINING,TESTING)\n",
    "\n",
    "    for j in [32,48,64,96,128,192,256,384,512,768,1024,1536,2048]:\n",
    "        pruned_weights = nn_pr_csc.getWeights()\n",
    "        nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=j,pruning=i,pre_weights=pruned_weights,stop_function=2,stop_parameter=0.02)\n",
    "        nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 14:47:27.071659) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Pruning: 10% (in)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m0s)   Accuracy TRAIN: 97.56%\tAccuracy TEST: 96.81%\tMin: 94.65% (7)\n",
      "Epoch 002 (2m0s)   Accuracy TRAIN: 97.97%\tAccuracy TEST: 97.19%\tMin: 95.62% (7)\n",
      "Epoch 003 (2m58s)  Accuracy TRAIN: 98.15%\tAccuracy TEST: 97.35%\tMin: 95.91% (7)\n",
      "Epoch 004 (3m57s)  Accuracy TRAIN: 98.26%\tAccuracy TEST: 97.49%\tMin: 96.11% (7)\n",
      "Epoch 005 (4m56s)  Accuracy TRAIN: 98.37%\tAccuracy TEST: 97.56%\tMin: 96.3% (7)\n",
      "Epoch 006 (5m55s)  Accuracy TRAIN: 98.44%\tAccuracy TEST: 97.58%\tMin: 96.4% (7)\n",
      "Epoch 007 (6m54s)  Accuracy TRAIN: 98.5%\tAccuracy TEST: 97.59%\tMin: 96.4% (7)\n",
      "Epoch 008 (7m53s)  Accuracy TRAIN: 98.54%\tAccuracy TEST: 97.63%\tMin: 96.3% (7)\n",
      "Epoch 009 (8m51s)  Accuracy TRAIN: 98.59%\tAccuracy TEST: 97.64%\tMin: 96.3% (7)\n",
      "Epoch 010 (9m50s)  Accuracy TRAIN: 98.63%\tAccuracy TEST: 97.66%\tMin: 96.3% (7)\n",
      "Epoch 011 (10m49s) Accuracy TRAIN: 98.65%\tAccuracy TEST: 97.68%\tMin: 96.3% (7)\n",
      "Epoch 012 (11m47s) Accuracy TRAIN: 98.67%\tAccuracy TEST: 97.69%\tMin: 96.4% (7)\n",
      "\n",
      "-- Training Session End (2019-01-23 14:59:14.702349) --\n",
      "-------------------------\n",
      "0.1,10.0,11m47s,98.67,97.69\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 14:59:14.858718) --\n",
      "\n",
      "Neurons: 300\n",
      "Pruning: 0.10\n",
      "Clusters: 3\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (4m4s)   Accuracy TRAIN: 82.02%\tAccuracy TEST: 80.83%\tMin: 31.58% (3)\n",
      "Epoch 002 (8m10s)  Accuracy TRAIN: 84.62%\tAccuracy TEST: 83.44%\tMin: 56.24% (3)\n",
      "Epoch 003 (12m13s) Accuracy TRAIN: 86.76%\tAccuracy TEST: 85.74%\tMin: 65.2% (8)\n",
      "Epoch 004 (16m14s) Accuracy TRAIN: 88.34%\tAccuracy TEST: 87.45%\tMin: 67.15% (8)\n",
      "Epoch 005 (20m19s) Accuracy TRAIN: 86.95%\tAccuracy TEST: 86.01%\tMin: 62.22% (8)\n",
      "\n",
      "-- Training Session End (2019-01-23 15:19:34.715167) --\n",
      "-------------------------\n",
      "0.1,3,401.726,20m19s,86.95,86.01\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 15:19:34.850842) --\n",
      "\n",
      "Neurons: 300\n",
      "Pruning: 0.10\n",
      "Clusters: 5\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (4m1s)   Accuracy TRAIN: 95.81%\tAccuracy TEST: 95.01%\tMin: 88.12% (3)\n",
      "Epoch 002 (8m2s)   Accuracy TRAIN: 96.2%\tAccuracy TEST: 95.52%\tMin: 91.08% (9)\n",
      "Epoch 003 (12m4s)  Accuracy TRAIN: 95.96%\tAccuracy TEST: 95.33%\tMin: 89.7% (3)\n",
      "\n",
      "-- Training Session End (2019-01-23 15:31:39.155249) --\n",
      "-------------------------\n",
      "0.1,5,274.027,12m4s,95.96,95.33\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 15:31:39.305647) --\n",
      "\n",
      "Neurons: 300\n",
      "Pruning: 0.10\n",
      "Clusters: 10\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (4m1s)   Accuracy TRAIN: 97.78%\tAccuracy TEST: 96.5%\tMin: 94.85% (3)\n",
      "Epoch 002 (8m7s)   Accuracy TRAIN: 97.18%\tAccuracy TEST: 96.02%\tMin: 93.58% (7)\n",
      "\n",
      "-- Training Session End (2019-01-23 15:39:46.864165) --\n",
      "-------------------------\n",
      "0.1,10,191.094,8m7s,97.18,96.02\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 15:39:47.156931) --\n",
      "\n",
      "Neurons: 300\n",
      "Pruning: 0.10\n",
      "Clusters: 16\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m58s)  Accuracy TRAIN: 98.21%\tAccuracy TEST: 96.98%\tMin: 95.04% (9)\n",
      "Epoch 002 (7m58s)  Accuracy TRAIN: 98.29%\tAccuracy TEST: 97.04%\tMin: 95.24% (9)\n",
      "Epoch 003 (12m13s) Accuracy TRAIN: 98.38%\tAccuracy TEST: 97.13%\tMin: 95.29% (5)\n",
      "Epoch 004 (16m28s) Accuracy TRAIN: 98.42%\tAccuracy TEST: 97.37%\tMin: 95.34% (9)\n",
      "Epoch 005 (20m39s) Accuracy TRAIN: 98.5%\tAccuracy TEST: 97.33%\tMin: 95.74% (9)\n",
      "Epoch 006 (24m53s) Accuracy TRAIN: 98.51%\tAccuracy TEST: 97.37%\tMin: 95.74% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 16:04:41.128560) --\n",
      "-------------------------\n",
      "0.1,16,158.277,24m53s,98.51,97.37\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_trained_weights = nn.getWeights()\n",
    "nn_pr_csc = Neural_Network_PR_CSC(neurons=300,batchsize=1,weights=pre_trained_weights,pruning=10,pruning_method='in',stop_function=2,stop_parameter=0.02)\n",
    "nn_pr_csc.train(TRAINING,TESTING)\n",
    "\n",
    "for i in [3,5,10,16]:\n",
    "    pruned_weights = nn_pr_csc.getWeights()\n",
    "    nn_pr_km = Neural_Network_PR_KM(neurons=300,batchsize=1,cluster=i,pruning=10,pre_weights=pruned_weights,stop_function=2,stop_parameter=0.02)\n",
    "    nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 16:16:57.578723) --\n",
      "\n",
      "Neurons: 100\n",
      "Batch Train: 60000\n",
      "Batch Test: 10000\n",
      "Stop Function: improvements below 0.01%\n",
      "\n",
      "Epoch 001 (17s)    Accuracy TRAIN: 92.84%\tAccuracy TEST: 93.04%\tMin: 85.65% (5)\n",
      "Epoch 010 (2m59s)  Accuracy TRAIN: 98.01%\tAccuracy TEST: 97.03%\tMin: 95.14% (9)\n",
      "Epoch 020 (5m56s)  Accuracy TRAIN: 98.64%\tAccuracy TEST: 97.39%\tMin: 95.84% (9)\n",
      "Epoch 025 (7m24s)  Accuracy TRAIN: 98.77%\tAccuracy TEST: 97.39%\tMin: 96.13% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 16:24:22.539351) --\n"
     ]
    }
   ],
   "source": [
    "nn100 = Neural_Network(neurons=100,batchsize=1,stop_function=2,stop_parameter=0.01)\n",
    "nn100.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 18:53:14.628922) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 32\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m4s)   Accuracy TRAIN: 11.22%\tAccuracy TEST: 11.32%\tMin: 00.0% (0)\n",
      "Epoch 002 (2m7s)   Accuracy TRAIN: 11.23%\tAccuracy TEST: 11.33%\tMin: 00.0% (0)\n",
      "\n",
      "-- Training Session End (2019-01-23 18:55:22.083035) --\n",
      "-------------------------\n",
      "32,12.778,2m7s,11.23,11.33\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 18:55:22.806599) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 48\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m2s)   Accuracy TRAIN: 28.5%\tAccuracy TEST: 28.32%\tMin: 00.0% (0)\n",
      "Epoch 002 (2m6s)   Accuracy TRAIN: 28.98%\tAccuracy TEST: 28.82%\tMin: 00.0% (0)\n",
      "Epoch 003 (3m8s)   Accuracy TRAIN: 98.68%\tAccuracy TEST: 97.16%\tMin: 94.65% (9)\n",
      "Epoch 004 (4m10s)  Accuracy TRAIN: 98.72%\tAccuracy TEST: 97.15%\tMin: 94.85% (9)\n",
      "Epoch 005 (5m15s)  Accuracy TRAIN: 98.7%\tAccuracy TEST: 97.14%\tMin: 94.85% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:00:38.185991) --\n",
      "-------------------------\n",
      "48,11.433,5m15s,98.7,97.14\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 19:00:38.985990) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 64\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m3s)   Accuracy TRAIN: 98.14%\tAccuracy TEST: 96.69%\tMin: 94.06% (5)\n",
      "Epoch 002 (2m9s)   Accuracy TRAIN: 98.41%\tAccuracy TEST: 96.79%\tMin: 95.04% (9)\n",
      "Epoch 003 (3m13s)  Accuracy TRAIN: 98.24%\tAccuracy TEST: 96.6%\tMin: 94.28% (5)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:03:52.297764) --\n",
      "-------------------------\n",
      "64,10.636,3m13s,98.24,96.6\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 19:03:52.908295) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 96\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m4s)   Accuracy TRAIN: 98.49%\tAccuracy TEST: 97.07%\tMin: 94.75% (9)\n",
      "Epoch 002 (2m6s)   Accuracy TRAIN: 98.64%\tAccuracy TEST: 97.06%\tMin: 94.65% (9)\n",
      "Epoch 003 (3m10s)  Accuracy TRAIN: 98.63%\tAccuracy TEST: 97.04%\tMin: 94.25% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:07:03.879436) --\n",
      "-------------------------\n",
      "96,9.681,3m10s,98.63,97.04\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 19:07:04.693601) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 128\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m3s)   Accuracy TRAIN: 98.59%\tAccuracy TEST: 97.09%\tMin: 94.45% (9)\n",
      "Epoch 002 (2m5s)   Accuracy TRAIN: 98.66%\tAccuracy TEST: 97.12%\tMin: 94.75% (9)\n",
      "Epoch 003 (3m8s)   Accuracy TRAIN: 98.72%\tAccuracy TEST: 97.21%\tMin: 94.85% (9)\n",
      "Epoch 004 (4m10s)  Accuracy TRAIN: 98.75%\tAccuracy TEST: 97.24%\tMin: 94.85% (9)\n",
      "Epoch 005 (5m13s)  Accuracy TRAIN: 98.76%\tAccuracy TEST: 97.25%\tMin: 94.45% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:12:18.297610) --\n",
      "-------------------------\n",
      "128,9.098,5m13s,98.76,97.25\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 19:12:19.216538) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 192\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m4s)   Accuracy TRAIN: 98.64%\tAccuracy TEST: 97.15%\tMin: 95.14% (9)\n",
      "Epoch 002 (2m8s)   Accuracy TRAIN: 98.69%\tAccuracy TEST: 97.15%\tMin: 94.65% (9)\n",
      "Epoch 003 (3m11s)  Accuracy TRAIN: 98.71%\tAccuracy TEST: 97.12%\tMin: 94.55% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:15:31.122920) --\n",
      "-------------------------\n",
      "192,8.38,3m11s,98.71,97.12\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 19:15:32.129477) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 256\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m3s)   Accuracy TRAIN: 98.66%\tAccuracy TEST: 97.06%\tMin: 95.44% (9)\n",
      "Epoch 002 (2m7s)   Accuracy TRAIN: 98.73%\tAccuracy TEST: 97.21%\tMin: 94.75% (9)\n",
      "Epoch 003 (3m11s)  Accuracy TRAIN: 98.76%\tAccuracy TEST: 97.17%\tMin: 94.85% (9)\n",
      "Epoch 004 (4m14s)  Accuracy TRAIN: 98.75%\tAccuracy TEST: 97.14%\tMin: 94.65% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:19:47.106966) --\n",
      "-------------------------\n",
      "256,7.931,4m14s,98.75,97.14\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Moris/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning: init_size=300 should be larger than k=384. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 19:19:48.533350) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 384\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m5s)   Accuracy TRAIN: 98.75%\tAccuracy TEST: 97.25%\tMin: 94.65% (9)\n",
      "Epoch 002 (2m10s)  Accuracy TRAIN: 98.8%\tAccuracy TEST: 97.21%\tMin: 94.85% (9)\n",
      "Epoch 003 (3m15s)  Accuracy TRAIN: 98.83%\tAccuracy TEST: 97.3%\tMin: 95.04% (9)\n",
      "Epoch 004 (4m21s)  Accuracy TRAIN: 98.84%\tAccuracy TEST: 97.32%\tMin: 95.14% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:24:09.796086) --\n",
      "-------------------------\n",
      "384,7.365,4m21s,98.84,97.32\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Moris/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning: init_size=300 should be larger than k=512. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 19:24:11.789501) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 512\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m6s)   Accuracy TRAIN: 98.76%\tAccuracy TEST: 97.25%\tMin: 95.74% (9)\n",
      "Epoch 002 (2m13s)  Accuracy TRAIN: 98.84%\tAccuracy TEST: 97.33%\tMin: 95.14% (9)\n",
      "Epoch 003 (3m20s)  Accuracy TRAIN: 98.86%\tAccuracy TEST: 97.36%\tMin: 95.24% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:27:32.588132) --\n",
      "-------------------------\n",
      "512,7.003,3m20s,98.86,97.36\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Moris/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning: init_size=300 should be larger than k=768. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 19:27:35.383694) --\n",
      "\n",
      "Neurons: 100\n",
      "Clusters: 768\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (1m10s)  Accuracy TRAIN: 98.75%\tAccuracy TEST: 97.33%\tMin: 95.84% (9)\n",
      "Epoch 002 (2m21s)  Accuracy TRAIN: 98.85%\tAccuracy TEST: 97.35%\tMin: 95.54% (9)\n",
      "Epoch 003 (3m31s)  Accuracy TRAIN: 98.88%\tAccuracy TEST: 97.37%\tMin: 95.64% (9)\n",
      "Epoch 004 (4m42s)  Accuracy TRAIN: 98.91%\tAccuracy TEST: 97.36%\tMin: 95.54% (9)\n",
      "Epoch 005 (5m53s)  Accuracy TRAIN: 98.92%\tAccuracy TEST: 97.38%\tMin: 95.64% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 19:33:28.422393) --\n",
      "-------------------------\n",
      "768,6.535,5m53s,98.92,97.38\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Moris/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning: init_size=300 should be larger than k=1024. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of samples smaller than number of clusters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-287-2ea2c6b1edb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1536\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpre_trained_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn100\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnn_km\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network_KM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_trained_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_parameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnn_km\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTESTING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-254-1282e4af9d3a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, neurons, batchsize, cluster, pre_weights, stop_function, stop_parameter)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Initialize cluster for pre-trained weights (dict with centers)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_output\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-478620ca2dfc>\u001b[0m in \u001b[0;36mbuild_clusters\u001b[0;34m(cluster, weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRANDOM_SEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1355\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m             raise ValueError(\"Number of samples smaller than number \"\n\u001b[0m\u001b[1;32m   1358\u001b[0m                              \"of clusters.\")\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of samples smaller than number of clusters."
     ]
    }
   ],
   "source": [
    "for i in [32,48,64,96,128,192,256,384,512,768,1024,1536,2048]:\n",
    "    pre_trained_weights = nn100.getWeights()\n",
    "    nn_km = Neural_Network_KM(neurons=100,batchsize=1,cluster=i,pre_weights=pre_trained_weights,stop_function=2,stop_parameter=0.02)\n",
    "    nn_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 20:39:24.657169) --\n",
      "\n",
      "Neurons: 100\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Pruning: 60% (in)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (37s)    Accuracy TRAIN: 98.62%\tAccuracy TEST: 97.41%\tMin: 95.72% (7)\n",
      "Epoch 002 (1m14s)  Accuracy TRAIN: 98.78%\tAccuracy TEST: 97.44%\tMin: 96.11% (7)\n",
      "Epoch 003 (1m52s)  Accuracy TRAIN: 98.88%\tAccuracy TEST: 97.49%\tMin: 96.23% (9)\n",
      "Epoch 004 (2m29s)  Accuracy TRAIN: 98.95%\tAccuracy TEST: 97.5%\tMin: 95.94% (9)\n",
      "Epoch 005 (3m7s)   Accuracy TRAIN: 99.01%\tAccuracy TEST: 97.51%\tMin: 95.94% (9)\n",
      "Epoch 006 (3m44s)  Accuracy TRAIN: 99.04%\tAccuracy TEST: 97.49%\tMin: 95.84% (9)\n",
      "Epoch 007 (4m21s)  Accuracy TRAIN: 99.09%\tAccuracy TEST: 97.5%\tMin: 95.84% (9)\n",
      "Epoch 008 (4m59s)  Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.49%\tMin: 95.84% (9)\n",
      "Epoch 009 (5m36s)  Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.52%\tMin: 96.04% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 20:45:01.309127) --\n",
      "-------------------------\n",
      "0.6,1.667,5m36s,99.14,97.52\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 20:45:01.640856) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 32\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m2s)   Accuracy TRAIN: 98.8%\tAccuracy TEST: 97.0%\tMin: 95.54% (9)\n",
      "Epoch 002 (6m5s)   Accuracy TRAIN: 98.69%\tAccuracy TEST: 96.86%\tMin: 94.15% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 20:51:07.189116) --\n",
      "-------------------------\n",
      "0.6,32,21.272,6m5s,98.69,96.86\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 20:51:07.536022) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 48\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m1s)   Accuracy TRAIN: 98.93%\tAccuracy TEST: 97.16%\tMin: 94.35% (9)\n",
      "Epoch 002 (6m3s)   Accuracy TRAIN: 98.93%\tAccuracy TEST: 97.1%\tMin: 94.05% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 20:57:11.269534) --\n",
      "-------------------------\n",
      "0.6,48,19.025,6m3s,98.93,97.1\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 20:57:11.655722) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 64\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m2s)   Accuracy TRAIN: 98.89%\tAccuracy TEST: 97.19%\tMin: 94.65% (9)\n",
      "Epoch 002 (6m4s)   Accuracy TRAIN: 99.03%\tAccuracy TEST: 97.18%\tMin: 94.75% (9)\n",
      "Epoch 003 (9m6s)   Accuracy TRAIN: 98.99%\tAccuracy TEST: 97.15%\tMin: 94.75% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:06:18.506742) --\n",
      "-------------------------\n",
      "0.6,64,17.692,9m6s,98.99,97.15\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 21:06:18.956320) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 96\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m2s)   Accuracy TRAIN: 99.07%\tAccuracy TEST: 97.28%\tMin: 94.85% (9)\n",
      "Epoch 002 (6m4s)   Accuracy TRAIN: 99.08%\tAccuracy TEST: 97.26%\tMin: 95.04% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:12:23.667106) --\n",
      "-------------------------\n",
      "0.6,96,16.092,6m4s,99.08,97.26\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 21:12:24.242109) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 128\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m2s)   Accuracy TRAIN: 99.07%\tAccuracy TEST: 97.31%\tMin: 94.85% (9)\n",
      "Epoch 002 (6m5s)   Accuracy TRAIN: 99.09%\tAccuracy TEST: 97.43%\tMin: 94.75% (9)\n",
      "Epoch 003 (9m8s)   Accuracy TRAIN: 99.1%\tAccuracy TEST: 97.37%\tMin: 94.95% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:21:32.385256) --\n",
      "-------------------------\n",
      "0.6,128,15.113,9m8s,99.1,97.37\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 21:21:33.038898) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 192\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m3s)   Accuracy TRAIN: 99.09%\tAccuracy TEST: 97.41%\tMin: 94.45% (9)\n",
      "Epoch 002 (6m6s)   Accuracy TRAIN: 99.13%\tAccuracy TEST: 97.42%\tMin: 94.55% (9)\n",
      "Epoch 003 (9m10s)  Accuracy TRAIN: 99.13%\tAccuracy TEST: 97.39%\tMin: 94.75% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:30:43.383065) --\n",
      "-------------------------\n",
      "0.6,192,13.903,9m10s,99.13,97.39\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 21:30:44.146839) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 256\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m3s)   Accuracy TRAIN: 99.1%\tAccuracy TEST: 97.45%\tMin: 95.14% (9)\n",
      "Epoch 002 (6m7s)   Accuracy TRAIN: 99.13%\tAccuracy TEST: 97.45%\tMin: 94.85% (9)\n",
      "Epoch 003 (9m11s)  Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.5%\tMin: 94.95% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:39:55.422929) --\n",
      "-------------------------\n",
      "0.6,256,13.143,9m11s,99.14,97.5\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Moris/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning: init_size=300 should be larger than k=384. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 21:39:56.514881) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 384\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m6s)   Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.46%\tMin: 95.44% (9)\n",
      "Epoch 002 (6m12s)  Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.43%\tMin: 95.54% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:46:09.275219) --\n",
      "-------------------------\n",
      "0.6,384,12.178,6m12s,99.14,97.43\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Moris/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:1418: RuntimeWarning: init_size=300 should be larger than k=512. Setting it to 3*k\n",
      "  init_size=init_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 21:46:10.836734) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.60\n",
      "Clusters: 512\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m7s)   Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.38%\tMin: 95.74% (9)\n",
      "Epoch 002 (6m14s)  Accuracy TRAIN: 99.16%\tAccuracy TEST: 97.41%\tMin: 95.64% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:52:25.336038) --\n",
      "-------------------------\n",
      "0.6,512,11.554,6m14s,99.16,97.41\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 21:52:25.369644) --\n",
      "\n",
      "Neurons: 100\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Pruning: 70% (in)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (38s)    Accuracy TRAIN: 98.6%\tAccuracy TEST: 97.38%\tMin: 95.53% (7)\n",
      "Epoch 002 (1m17s)  Accuracy TRAIN: 98.78%\tAccuracy TEST: 97.53%\tMin: 96.11% (7)\n",
      "Epoch 003 (1m55s)  Accuracy TRAIN: 98.89%\tAccuracy TEST: 97.56%\tMin: 96.21% (7)\n",
      "Epoch 004 (2m34s)  Accuracy TRAIN: 98.97%\tAccuracy TEST: 97.54%\tMin: 96.23% (9)\n",
      "Epoch 005 (3m13s)  Accuracy TRAIN: 99.02%\tAccuracy TEST: 97.54%\tMin: 96.13% (9)\n",
      "Epoch 006 (3m51s)  Accuracy TRAIN: 99.06%\tAccuracy TEST: 97.55%\tMin: 96.13% (9)\n",
      "Epoch 007 (4m30s)  Accuracy TRAIN: 99.1%\tAccuracy TEST: 97.53%\tMin: 96.13% (9)\n",
      "Epoch 008 (5m8s)   Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.55%\tMin: 96.33% (9)\n",
      "Epoch 009 (5m47s)  Accuracy TRAIN: 99.15%\tAccuracy TEST: 97.57%\tMin: 96.23% (9)\n",
      "Epoch 010 (6m26s)  Accuracy TRAIN: 99.16%\tAccuracy TEST: 97.57%\tMin: 96.23% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 21:58:51.560667) --\n",
      "-------------------------\n",
      "0.7,1.429,6m26s,99.16,97.57\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 21:58:51.931202) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 32\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m16s)  Accuracy TRAIN: 09.4%\tAccuracy TEST: 8.77%\tMin: 00.0% (1)\n",
      "Epoch 002 (6m33s)  Accuracy TRAIN: 9.47%\tAccuracy TEST: 8.86%\tMin: 00.0% (1)\n",
      "Epoch 003 (9m49s)  Accuracy TRAIN: 9.48%\tAccuracy TEST: 8.85%\tMin: 00.0% (1)\n",
      "\n",
      "-- Training Session End (2019-01-23 22:08:41.627441) --\n",
      "-------------------------\n",
      "0.7,32,18.24,9m49s,9.48,8.85\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 22:08:42.022313) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 48\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m17s)  Accuracy TRAIN: 98.46%\tAccuracy TEST: 96.7%\tMin: 93.46% (9)\n",
      "Epoch 002 (6m34s)  Accuracy TRAIN: 98.9%\tAccuracy TEST: 97.04%\tMin: 94.15% (9)\n",
      "Epoch 003 (9m51s)  Accuracy TRAIN: 98.9%\tAccuracy TEST: 97.07%\tMin: 94.65% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 22:18:33.980194) --\n",
      "-------------------------\n",
      "0.7,48,16.316,9m51s,98.9,97.07\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 22:18:34.417526) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 64\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m17s)  Accuracy TRAIN: 99.02%\tAccuracy TEST: 97.25%\tMin: 94.65% (9)\n",
      "Epoch 002 (6m34s)  Accuracy TRAIN: 99.01%\tAccuracy TEST: 97.21%\tMin: 94.35% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 22:25:09.399931) --\n",
      "-------------------------\n",
      "0.7,64,15.175,6m34s,99.01,97.21\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 22:25:09.891501) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 96\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m17s)  Accuracy TRAIN: 99.01%\tAccuracy TEST: 97.15%\tMin: 94.35% (9)\n",
      "Epoch 002 (6m36s)  Accuracy TRAIN: 99.06%\tAccuracy TEST: 97.36%\tMin: 94.25% (9)\n",
      "Epoch 003 (9m53s)  Accuracy TRAIN: 99.11%\tAccuracy TEST: 97.38%\tMin: 94.65% (9)\n",
      "Epoch 004 (13m11s) Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.45%\tMin: 94.65% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 22:38:21.334174) --\n",
      "-------------------------\n",
      "0.7,96,13.806,13m11s,99.12,97.45\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 22:38:21.903623) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 128\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m18s)  Accuracy TRAIN: 99.07%\tAccuracy TEST: 97.14%\tMin: 94.65% (9)\n",
      "Epoch 002 (6m35s)  Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.28%\tMin: 94.75% (9)\n",
      "Epoch 003 (9m53s)  Accuracy TRAIN: 99.11%\tAccuracy TEST: 97.29%\tMin: 94.65% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 22:48:15.766193) --\n",
      "-------------------------\n",
      "0.7,128,12.969,9m53s,99.11,97.29\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 22:48:16.465088) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 192\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m18s)  Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.31%\tMin: 94.95% (9)\n",
      "Epoch 002 (6m37s)  Accuracy TRAIN: 99.18%\tAccuracy TEST: 97.37%\tMin: 95.24% (9)\n",
      "Epoch 003 (9m56s)  Accuracy TRAIN: 99.18%\tAccuracy TEST: 97.4%\tMin: 95.14% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 22:58:13.240870) --\n",
      "-------------------------\n",
      "0.7,192,11.936,9m56s,99.18,97.4\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 22:58:14.718895) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 256\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m17s)  Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.24%\tMin: 94.65% (9)\n",
      "Epoch 002 (6m34s)  Accuracy TRAIN: 99.18%\tAccuracy TEST: 97.33%\tMin: 94.85% (9)\n",
      "Epoch 003 (9m51s)  Accuracy TRAIN: 99.18%\tAccuracy TEST: 97.34%\tMin: 95.04% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 23:08:06.314037) --\n",
      "-------------------------\n",
      "0.7,256,11.288,9m51s,99.18,97.34\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 23:08:07.507708) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 384\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m20s)  Accuracy TRAIN: 99.16%\tAccuracy TEST: 97.45%\tMin: 95.34% (9)\n",
      "Epoch 002 (6m40s)  Accuracy TRAIN: 99.19%\tAccuracy TEST: 97.4%\tMin: 95.04% (9)\n",
      "Epoch 003 (10m1s)  Accuracy TRAIN: 99.2%\tAccuracy TEST: 97.4%\tMin: 95.04% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 23:18:08.690167) --\n",
      "-------------------------\n",
      "0.7,384,10.468,10m1s,99.2,97.4\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 23:18:10.374275) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.70\n",
      "Clusters: 512\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m28s)  Accuracy TRAIN: 99.19%\tAccuracy TEST: 97.39%\tMin: 95.64% (9)\n",
      "Epoch 002 (6m53s)  Accuracy TRAIN: 99.21%\tAccuracy TEST: 97.4%\tMin: 95.54% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 23:25:03.852419) --\n",
      "-------------------------\n",
      "0.7,512,9.939,6m53s,99.21,97.4\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 23:25:03.885580) --\n",
      "\n",
      "Neurons: 100\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Pruning: 80% (in)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (39s)    Accuracy TRAIN: 98.6%\tAccuracy TEST: 97.37%\tMin: 95.72% (7)\n",
      "Epoch 002 (1m20s)  Accuracy TRAIN: 98.78%\tAccuracy TEST: 97.46%\tMin: 96.01% (7)\n",
      "Epoch 003 (2m0s)   Accuracy TRAIN: 98.88%\tAccuracy TEST: 97.53%\tMin: 96.3% (7)\n",
      "Epoch 004 (2m40s)  Accuracy TRAIN: 98.96%\tAccuracy TEST: 97.55%\tMin: 96.3% (7)\n",
      "Epoch 005 (3m20s)  Accuracy TRAIN: 99.01%\tAccuracy TEST: 97.55%\tMin: 96.4% (7)\n",
      "Epoch 006 (4m0s)   Accuracy TRAIN: 99.07%\tAccuracy TEST: 97.57%\tMin: 96.43% (9)\n",
      "Epoch 007 (4m40s)  Accuracy TRAIN: 99.1%\tAccuracy TEST: 97.58%\tMin: 96.53% (9)\n",
      "Epoch 008 (5m19s)  Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.58%\tMin: 96.53% (9)\n",
      "Epoch 009 (5m59s)  Accuracy TRAIN: 99.16%\tAccuracy TEST: 97.63%\tMin: 96.53% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 23:31:03.807122) --\n",
      "-------------------------\n",
      "0.8,1.25,5m59s,99.16,97.63\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 23:31:04.218870) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 32\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m34s)  Accuracy TRAIN: 98.7%\tAccuracy TEST: 97.01%\tMin: 94.35% (9)\n",
      "Epoch 002 (7m9s)   Accuracy TRAIN: 98.74%\tAccuracy TEST: 97.12%\tMin: 94.75% (9)\n",
      "Epoch 003 (10m43s) Accuracy TRAIN: 98.71%\tAccuracy TEST: 97.07%\tMin: 94.55% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 23:41:47.587513) --\n",
      "-------------------------\n",
      "0.8,32,15.965,10m43s,98.71,97.07\n",
      "-------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2019-01-23 23:41:48.044974) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 48\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m36s)  Accuracy TRAIN: 98.74%\tAccuracy TEST: 96.76%\tMin: 93.95% (9)\n",
      "Epoch 002 (7m15s)  Accuracy TRAIN: 98.92%\tAccuracy TEST: 97.07%\tMin: 94.55% (9)\n",
      "Epoch 003 (10m50s) Accuracy TRAIN: 98.95%\tAccuracy TEST: 97.08%\tMin: 94.45% (9)\n",
      "Epoch 004 (14m26s) Accuracy TRAIN: 98.94%\tAccuracy TEST: 97.15%\tMin: 94.65% (9)\n",
      "\n",
      "-- Training Session End (2019-01-23 23:56:14.354980) --\n",
      "-------------------------\n",
      "0.8,48,14.282,14m26s,98.94,97.15\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-23 23:56:14.842736) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 64\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m35s)  Accuracy TRAIN: 98.88%\tAccuracy TEST: 97.19%\tMin: 95.34% (9)\n",
      "Epoch 002 (7m10s)  Accuracy TRAIN: 98.92%\tAccuracy TEST: 97.16%\tMin: 95.14% (9)\n",
      "Epoch 003 (10m45s) Accuracy TRAIN: 98.92%\tAccuracy TEST: 97.21%\tMin: 95.04% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 00:07:00.499223) --\n",
      "-------------------------\n",
      "0.8,64,13.285,10m45s,98.92,97.21\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 00:07:01.044442) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 96\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m34s)  Accuracy TRAIN: 98.99%\tAccuracy TEST: 97.27%\tMin: 95.14% (9)\n",
      "Epoch 002 (7m7s)   Accuracy TRAIN: 99.03%\tAccuracy TEST: 97.22%\tMin: 95.14% (9)\n",
      "Epoch 003 (10m41s) Accuracy TRAIN: 99.02%\tAccuracy TEST: 97.3%\tMin: 95.14% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 00:17:42.108613) --\n",
      "-------------------------\n",
      "0.8,96,12.089,10m41s,99.02,97.3\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 00:17:42.715893) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 128\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m33s)  Accuracy TRAIN: 99.08%\tAccuracy TEST: 97.37%\tMin: 95.44% (9)\n",
      "Epoch 002 (7m7s)   Accuracy TRAIN: 99.11%\tAccuracy TEST: 97.34%\tMin: 94.95% (9)\n",
      "Epoch 003 (10m41s) Accuracy TRAIN: 99.13%\tAccuracy TEST: 97.39%\tMin: 94.95% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 00:28:23.886613) --\n",
      "-------------------------\n",
      "0.8,128,11.358,10m41s,99.13,97.39\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 00:28:24.628263) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 192\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m34s)  Accuracy TRAIN: 99.06%\tAccuracy TEST: 97.25%\tMin: 95.14% (9)\n",
      "Epoch 002 (7m9s)   Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.27%\tMin: 95.24% (9)\n",
      "Epoch 003 (10m43s) Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.3%\tMin: 95.14% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 00:39:08.114610) --\n",
      "-------------------------\n",
      "0.8,192,10.457,10m43s,99.14,97.3\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 00:39:09.075257) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 256\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m34s)  Accuracy TRAIN: 99.1%\tAccuracy TEST: 97.36%\tMin: 95.54% (9)\n",
      "Epoch 002 (7m8s)   Accuracy TRAIN: 99.13%\tAccuracy TEST: 97.28%\tMin: 95.14% (9)\n",
      "Epoch 003 (10m43s) Accuracy TRAIN: 99.15%\tAccuracy TEST: 97.31%\tMin: 95.34% (9)\n",
      "Epoch 004 (14m17s) Accuracy TRAIN: 99.16%\tAccuracy TEST: 97.31%\tMin: 95.24% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 00:53:26.990023) --\n",
      "-------------------------\n",
      "0.8,256,9.892,14m17s,99.16,97.31\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 00:53:28.245817) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 384\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m35s)  Accuracy TRAIN: 99.13%\tAccuracy TEST: 97.34%\tMin: 95.14% (9)\n",
      "Epoch 002 (7m11s)  Accuracy TRAIN: 99.18%\tAccuracy TEST: 97.49%\tMin: 95.24% (9)\n",
      "Epoch 003 (10m46s) Accuracy TRAIN: 99.2%\tAccuracy TEST: 97.48%\tMin: 95.44% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 01:04:15.165070) --\n",
      "-------------------------\n",
      "0.8,384,9.179,10m46s,99.2,97.48\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 01:04:16.886435) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.80\n",
      "Clusters: 512\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m38s)  Accuracy TRAIN: 99.09%\tAccuracy TEST: 97.47%\tMin: 95.38% (8)\n",
      "Epoch 002 (7m16s)  Accuracy TRAIN: 99.2%\tAccuracy TEST: 97.44%\tMin: 95.34% (9)\n",
      "Epoch 003 (10m54s) Accuracy TRAIN: 99.2%\tAccuracy TEST: 97.42%\tMin: 95.34% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 01:15:11.155205) --\n",
      "-------------------------\n",
      "0.8,512,8.72,10m54s,99.2,97.42\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 01:15:11.186439) --\n",
      "\n",
      "Neurons: 100\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Pruning: 90% (in)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (39s)    Accuracy TRAIN: 98.6%\tAccuracy TEST: 97.4%\tMin: 95.72% (7)\n",
      "Epoch 002 (1m18s)  Accuracy TRAIN: 98.78%\tAccuracy TEST: 97.51%\tMin: 96.21% (7)\n",
      "Epoch 003 (1m58s)  Accuracy TRAIN: 98.89%\tAccuracy TEST: 97.49%\tMin: 96.3% (7)\n",
      "Epoch 004 (2m37s)  Accuracy TRAIN: 98.97%\tAccuracy TEST: 97.51%\tMin: 96.3% (7)\n",
      "Epoch 005 (3m17s)  Accuracy TRAIN: 99.02%\tAccuracy TEST: 97.52%\tMin: 96.5% (7)\n",
      "Epoch 006 (3m56s)  Accuracy TRAIN: 99.07%\tAccuracy TEST: 97.56%\tMin: 96.6% (7)\n",
      "Epoch 007 (4m35s)  Accuracy TRAIN: 99.11%\tAccuracy TEST: 97.53%\tMin: 96.43% (9)\n",
      "Epoch 008 (5m15s)  Accuracy TRAIN: 99.14%\tAccuracy TEST: 97.56%\tMin: 96.53% (9)\n",
      "Epoch 009 (5m54s)  Accuracy TRAIN: 99.16%\tAccuracy TEST: 97.59%\tMin: 96.53% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 01:21:06.008333) --\n",
      "-------------------------\n",
      "0.9,1.111,5m54s,99.16,97.59\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 01:21:06.447935) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 32\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m46s)  Accuracy TRAIN: 22.0%\tAccuracy TEST: 22.94%\tMin: 00.0% (6)\n",
      "Epoch 002 (7m32s)  Accuracy TRAIN: 19.19%\tAccuracy TEST: 19.83%\tMin: 00.0% (1)\n",
      "\n",
      "-- Training Session End (2019-01-24 01:28:39.292189) --\n",
      "-------------------------\n",
      "0.9,32,14.195,7m32s,19.19,19.83\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 01:28:39.778468) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 48\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m38s)  Accuracy TRAIN: 98.84%\tAccuracy TEST: 96.92%\tMin: 94.85% (9)\n",
      "Epoch 002 (7m16s)  Accuracy TRAIN: 98.94%\tAccuracy TEST: 97.13%\tMin: 95.07% (8)\n",
      "Epoch 003 (10m55s) Accuracy TRAIN: 98.86%\tAccuracy TEST: 96.94%\tMin: 94.35% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 01:39:35.317910) --\n",
      "-------------------------\n",
      "0.9,48,12.7,10m55s,98.86,96.94\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 01:39:35.821449) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 64\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m39s)  Accuracy TRAIN: 98.7%\tAccuracy TEST: 97.0%\tMin: 94.95% (9)\n",
      "Epoch 002 (7m18s)  Accuracy TRAIN: 98.86%\tAccuracy TEST: 97.27%\tMin: 95.44% (9)\n",
      "Epoch 003 (10m57s) Accuracy TRAIN: 98.84%\tAccuracy TEST: 97.16%\tMin: 93.86% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 01:50:33.231864) --\n",
      "-------------------------\n",
      "0.9,64,11.814,10m57s,98.84,97.16\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 01:50:33.809835) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 96\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m38s)  Accuracy TRAIN: 98.97%\tAccuracy TEST: 97.34%\tMin: 95.04% (9)\n",
      "Epoch 002 (7m15s)  Accuracy TRAIN: 99.04%\tAccuracy TEST: 97.27%\tMin: 94.95% (9)\n",
      "Epoch 003 (10m54s) Accuracy TRAIN: 99.08%\tAccuracy TEST: 97.34%\tMin: 94.85% (9)\n",
      "Epoch 004 (14m32s) Accuracy TRAIN: 99.1%\tAccuracy TEST: 97.33%\tMin: 94.75% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 02:05:06.348816) --\n",
      "-------------------------\n",
      "0.9,96,10.752,14m32s,99.1,97.33\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 02:05:07.005841) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 128\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 (3m38s)  Accuracy TRAIN: 99.05%\tAccuracy TEST: 97.36%\tMin: 95.44% (9)\n",
      "Epoch 002 (7m16s)  Accuracy TRAIN: 99.1%\tAccuracy TEST: 97.28%\tMin: 94.95% (9)\n",
      "Epoch 003 (10m55s) Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.44%\tMin: 95.34% (9)\n",
      "Epoch 004 (14m33s) Accuracy TRAIN: 99.13%\tAccuracy TEST: 97.46%\tMin: 95.14% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 02:19:40.815446) --\n",
      "-------------------------\n",
      "0.9,128,10.103,14m33s,99.13,97.46\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 02:19:41.664084) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 192\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m39s)  Accuracy TRAIN: 99.0%\tAccuracy TEST: 97.32%\tMin: 95.54% (9)\n",
      "Epoch 002 (7m17s)  Accuracy TRAIN: 99.12%\tAccuracy TEST: 97.46%\tMin: 95.34% (9)\n",
      "Epoch 003 (10m56s) Accuracy TRAIN: 99.15%\tAccuracy TEST: 97.48%\tMin: 95.24% (9)\n",
      "Epoch 004 (14m35s) Accuracy TRAIN: 99.15%\tAccuracy TEST: 97.54%\tMin: 95.04% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 02:34:17.446832) --\n",
      "-------------------------\n",
      "0.9,192,9.304,14m35s,99.15,97.54\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 02:34:18.397638) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 256\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m40s)  Accuracy TRAIN: 99.09%\tAccuracy TEST: 97.31%\tMin: 96.2% (8)\n",
      "Epoch 002 (7m20s)  Accuracy TRAIN: 99.17%\tAccuracy TEST: 97.44%\tMin: 95.64% (9)\n",
      "Epoch 003 (11m1s)  Accuracy TRAIN: 99.17%\tAccuracy TEST: 97.4%\tMin: 95.34% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 02:45:19.897849) --\n",
      "-------------------------\n",
      "0.9,256,8.804,11m1s,99.17,97.4\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 02:45:21.165322) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 384\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m40s)  Accuracy TRAIN: 99.15%\tAccuracy TEST: 97.36%\tMin: 95.64% (9)\n",
      "Epoch 002 (7m21s)  Accuracy TRAIN: 99.19%\tAccuracy TEST: 97.42%\tMin: 95.34% (9)\n",
      "Epoch 003 (11m2s)  Accuracy TRAIN: 99.2%\tAccuracy TEST: 97.38%\tMin: 95.04% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 02:56:23.310811) --\n",
      "-------------------------\n",
      "0.9,384,8.173,11m2s,99.2,97.38\n",
      "-------------------------\n",
      "\n",
      "-- Training Session Start (2019-01-24 02:56:25.090289) --\n",
      "\n",
      "Neurons: 100\n",
      "Pruning: 0.90\n",
      "Clusters: 512\n",
      "Batch Train: 60000 (100%)\n",
      "Batch Test: 10000 (100%)\n",
      "Stop Function: improvements below 0.02%\n",
      "\n",
      "Epoch 001 (3m43s)  Accuracy TRAIN: 99.16%\tAccuracy TEST: 97.42%\tMin: 96.04% (9)\n",
      "Epoch 002 (7m57s)  Accuracy TRAIN: 99.19%\tAccuracy TEST: 97.5%\tMin: 95.74% (9)\n",
      "Epoch 003 (11m40s) Accuracy TRAIN: 99.2%\tAccuracy TEST: 97.55%\tMin: 95.74% (9)\n",
      "\n",
      "-- Training Session End (2019-01-24 03:08:05.841310) --\n",
      "-------------------------\n",
      "0.9,512,7.768,11m40s,99.2,97.55\n",
      "-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(60,100,10):\n",
    "    pre_trained_weights = nn100.getWeights()\n",
    "    nn_pr_csc = Neural_Network_PR_CSC(neurons=100,batchsize=1,weights=pre_trained_weights,pruning=i,pruning_method='in',stop_function=2,stop_parameter=0.02)\n",
    "    nn_pr_csc.train(TRAINING,TESTING)\n",
    "\n",
    "    for j in [32,48,64,96,128,192,256,384,512]:\n",
    "        pruned_weights = nn_pr_csc.getWeights()\n",
    "        nn_pr_km = Neural_Network_PR_KM(neurons=100,batchsize=1,cluster=j,pruning=i,pre_weights=pruned_weights,stop_function=2,stop_parameter=0.02)\n",
    "        nn_pr_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_name):\n",
    "    with open(file_name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        header = next(csv_reader, None)\n",
    "        data = np.array([[t for t in row] for row in csv_reader])\n",
    "        return header,data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = read_csv('experiments/csv/pruning.csv')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(p[1][0],p[1][4])\n",
    "plt.plot(p[1][0],p[1][3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = read_csv('experiments/pruning_clustering.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
