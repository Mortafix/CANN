{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK with K-MEANS for MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import gzip\n",
    "from random import randint\n",
    "from scipy import misc\n",
    "from scipy import special\n",
    "import scipy.ndimage\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing(f):\n",
    "    def wrap(*args):\n",
    "        time1 = time.time()\n",
    "        ret = f(*args)\n",
    "        time2 = time.time()\n",
    "        print('%s function took %0.2f ms' % (f.__name__, (time2-time1)*1000.0))\n",
    "        return ret\n",
    "    return wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/mnist/'\n",
    "\n",
    "IMAGES_TRAIN = 'data_training'\n",
    "IMAGES_TEST = 'data_testing'\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_CLASSES = 10\n",
    "N_FEATURES = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = DATA_PATH+IMAGES_TRAIN\n",
    "data_testing = DATA_PATH+IMAGES_TEST\n",
    "ft = gzip.open(data_training, 'rb')\n",
    "TRAINING = pickle.load(ft)\n",
    "ft.close()\n",
    "ft = gzip.open(data_testing, 'rb')\n",
    "TESTING = pickle.load(ft)\n",
    "ft.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network:\n",
    "\n",
    "    def __init__(self, neurons=100, epochs=0, batchsize=1):\n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        \n",
    "        # Decide training method (epochs or covergence)\n",
    "        self.epochs = epochs\n",
    "        if epochs == 0:\n",
    "            self.best = 0.\n",
    "            self.same = 0\n",
    "\n",
    "        # Standardize random weights\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        hidden_layer = np.random.rand(self.neurons, self.input_size + 1) / self.neurons\n",
    "        output_layer = np.random.rand(self.output_size, self.neurons + 1) / self.output_size\n",
    "        self.layers = [hidden_layer, output_layer]\n",
    "        self.iteration = 0\n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Epochs: \"+str(self.epochs)\n",
    "        if self.epochs == 0:\n",
    "            typeTrainingPrint = \"Until 5 iterations w/o improvements\";\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "\n",
    "        # Performs iterations\n",
    "        while (self.epochs == 0 and self.same < 10) or (self.iteration < self.epochs):\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                self.backpropagate(input_vector, target_vector)\n",
    "            \n",
    "            # Accuracy\n",
    "            accu = self.accu(test_output)\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Messages\n",
    "            if (self.iteration == 1 or self.iteration % 10 == 0 or self.iteration == self.epochs):\n",
    "                self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "            \n",
    "            # Control coverange\n",
    "            if self.epochs == 0:\n",
    "                if accu[0] > self.best:\n",
    "                    self.same = 0\n",
    "                    self.best = accu[0]\n",
    "                else:\n",
    "                    self.same += 1\n",
    "                \n",
    "        # Print last epoch in coverange situation\n",
    "        if self.iteration % 10 != 0:\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "\n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        \"\"\"Takes a network (Matrix list) and returns the outputs of both\n",
    "         layers by propagating the entry\"\"\"\n",
    "        outputs = []\n",
    "        for layer in self.layers:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(layer, input_with_bias)\n",
    "            output = special.expit(output)\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target):\n",
    "        \"\"\"Reduce error for one input vector:\n",
    "        Calculating the partial derivatives for each coeff then subtracts\"\"\"\n",
    "        c = 1./(self.iteration + 10)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        self.layers[-1] -= c*np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(self.layers[-1], 200, 1).T, output_deltas)\n",
    "        self.layers[0] -= c*np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "\n",
    "    def predict(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector):\n",
    "        return np.argmax(self.feed_forward(input_vector)[-1])\n",
    "\n",
    "    def accu(self, testing):\n",
    "        \"\"\"The lowest precision digit and total\"\"\"\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k]) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    def print_message_iter(self,iteration,accu,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Precision G:'+str(accu[1]).zfill(4)+'%\\tMin:'+ str(accu[0]).zfill(4)+ '% ('+str(int(accu[2]))+')'\n",
    "        print(message)\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training Session Start (2018-08-06 12:55:53.559420) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 6000\n",
      "Batch Test: 1000\n",
      "Until 5 iterations w/o improvements\n",
      "\n",
      "Epoch 001 (13s)    Precision G:80.6%\tMin:25.45% (5)\n",
      "Epoch 010 (1m48s)  Precision G:91.62%\tMin:86.14% (8)\n",
      "Epoch 020 (3m30s)  Precision G:92.65%\tMin:87.89% (5)\n",
      "Epoch 030 (5m16s)  Precision G:92.89%\tMin:88.45% (5)\n",
      "Epoch 040 (6m58s)  Precision G:93.05%\tMin:89.24% (5)\n",
      "Epoch 050 (8m41s)  Precision G:93.22%\tMin:89.8% (5)\n",
      "Epoch 060 (10m30s) Precision G:93.25%\tMin:89.8% (5)\n",
      "\n",
      "-- Training Session End (2018-08-06 13:06:24.343838) --\n"
     ]
    }
   ],
   "source": [
    "nn = Neural_Network(neurons=300,epochs=0,batchsize=0.1)\n",
    "nn.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-MEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find nearest centroid given a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_centroid_index(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def nearest_centroid(centers,value):\n",
    "    centers = np.asarray(centers)\n",
    "    idx = (np.abs(centers - value)).argmin()\n",
    "    return centers[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building clusters with pre trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -0.013316040707466855,\n",
       " 1: 0.17661912046167377,\n",
       " 2: -0.24357098510667816,\n",
       " 3: 0.05777779120476522,\n",
       " 4: -0.11883780024622521,\n",
       " 5: -0.052577244677297075,\n",
       " 6: 0.02449335684861153,\n",
       " 7: 0.10423147754188361,\n",
       " 8: 0.368992533428038,\n",
       " 9: 0.0028422439569752295}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_weights = nn.getWeights()\n",
    "\n",
    "def reshape_weights_for_kmeans(weights):\n",
    "    return np.hstack(weights).reshape(-1,1)\n",
    "\n",
    "def build_clusters(cluster,weights):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=cluster,random_state=RANDOM_SEED)\n",
    "    kmeans.fit(reshape_weights_for_kmeans(weights))\n",
    "    return {k:v[0] for k,v in enumerate(kmeans.cluster_centers_)}\n",
    "\n",
    "build_clusters(10,nn_weights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redifine weights matrix for pre train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redefine_weights(weights,centers):\n",
    "        arr_ret = np.empty_like(weights)\n",
    "        for i, row in enumerate(weights):\n",
    "            for j, col in enumerate(row):\n",
    "                arr_ret[i,j] = nearest_centroid_index(centers,weights[i,j])\n",
    "        return arr_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean of distance between elements in centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_distance(weights,centroids):\n",
    "    tot = 0.\n",
    "    for i, row in enumerate(weights):\n",
    "        for j, col in enumerate(row):\n",
    "            weight = weights[i,j]\n",
    "            centroid = nearest_centroid(centroids,weight)[0]\n",
    "            tot += np.sqrt((weight - centroid)**2)\n",
    "    return tot / ((i+1)*(j+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean with different clusters number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean for 001 clusters 0.031771 (0m2s) \n",
      "Mean for 011 clusters 0.006801 (0m5s) \n",
      "Mean for 021 clusters 0.003505 (0m7s) \n",
      "Mean for 031 clusters 0.002469 (0m10s) \n",
      "Mean for 041 clusters 0.002123 (0m13s) \n",
      "Mean for 051 clusters 0.001557 (0m15s) \n",
      "Mean for 061 clusters 0.001424 (0m18s) \n",
      "Mean for 071 clusters 0.001288 (0m21s) \n",
      "Mean for 081 clusters 0.000978 (0m28s) \n",
      "Mean for 091 clusters 0.000988 (0m31s) \n",
      "Mean for 101 clusters 0.000938 (0m34s) \n"
     ]
    }
   ],
   "source": [
    "def find_clusters_number(values,n_from,n_to,n_jump):\n",
    "    start_time = dt.datetime.now()\n",
    "    RANDOM_SEED = 42\n",
    "    result = {}\n",
    "    for i in range(n_from,n_to+1,n_jump):\n",
    "        kmeans = MiniBatchKMeans(n_clusters=i,random_state=RANDOM_SEED)\n",
    "        kmeans.fit(reshape_weights_for_kmeans(values))\n",
    "        mean = mean_distance(values,kmeans.cluster_centers_)\n",
    "        result[i] = mean\n",
    "        print(\"Mean for %s clusters %f (%s) \" % (str(i).zfill(3),mean,eta_from_start(start_time)))\n",
    "    return result\n",
    "\n",
    "def eta_from_start(start_time):\n",
    "    diff = dt.datetime.now() - start_time\n",
    "    eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "    return str(eta[0])+\"m\"+str(eta[1])+\"s\"\n",
    "        \n",
    "means_cluster_1_10 = find_clusters_number(nn_weights[0],1,101,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redefine matrix (index --> centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def values_from_dict(a,d):\n",
    "    new_array = np.copy(a)\n",
    "    for k, v in d.items():\n",
    "        new_array[a==k] = v\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate gradient for each cluster centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_gradient(idx_matrix,gradient,cluster):\n",
    "    return scipy.ndimage.sum(gradient,idx_matrix,index=range(cluster))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_data_from_json(file):\n",
    "    with open(file) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def get_list_values_from_json(data,field,variable):\n",
    "    return [element[field] for element in data[variable]]\n",
    "\n",
    "data = get_data_from_json('log/Log_Cluster_Setting_Mean.json')\n",
    "x = get_list_values_from_json(data,\"cluster\",\"cluster_mean\")\n",
    "y = get_list_values_from_json(data,\"mean\",\"cluster_mean\")\n",
    "\n",
    "plt.plot(x[:10],y[:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network with K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network_KM:\n",
    "\n",
    "    def __init__(self, neurons=100, epochs=0, batchsize=1, cluster=10, pre_weights=None, verbose=True):\n",
    "        \n",
    "        start_setting_time = dt.datetime.now()\n",
    "        \n",
    "        self.input_size = N_FEATURES\n",
    "        self.output_size = N_CLASSES\n",
    "        self.neurons = neurons\n",
    "        self.batchsize = batchsize\n",
    "        self.verbose = verbose\n",
    "        self.cluster = cluster\n",
    "        self.iteration = 0\n",
    "        \n",
    "        # Decide training method (epochs or convergence)\n",
    "        self.epochs = epochs\n",
    "        if epochs == 0:\n",
    "            self.best = 0.\n",
    "            self.same = 0\n",
    "            \n",
    "        # Initialize cluster for pre-trained weights\n",
    "        c_hidden = build_clusters(self.cluster,pre_weights[0])\n",
    "        c_output = build_clusters(self.cluster,pre_weights[-1])\n",
    "        self.centers = [c_hidden,c_output]\n",
    "        \n",
    "        # Initialize index matrix for pre-trained weights\n",
    "        idx_hidden = redefine_weights(pre_weights[0],list(c_hidden.values()))\n",
    "        idx_output = redefine_weights(pre_weights[-1],list(c_output.values()))\n",
    "        self.idx_layers = [idx_hidden,idx_output]\n",
    "        \n",
    "        # Setting time print    \n",
    "        end_setting_time = dt.datetime.now() - start_setting_time\n",
    "        eta = divmod(end_setting_time.days * 86400 + end_setting_time.seconds, 60)\n",
    "        self.eta_print_setting = str(eta[0])+\"m\"+str(eta[1])+\"s\"\n",
    "        if self.verbose:\n",
    "            print(\"--- Setting Time: %s ---\" % self.eta_print_setting)\n",
    "    \n",
    " \n",
    "\n",
    "    def train(self, training, testing):\n",
    "        \n",
    "        # Batch Setting\n",
    "        len_batch_train = len(training[0])\n",
    "        len_batch_test = len(testing[0])\n",
    "        if(self.batchsize > 0 and self.batchsize <= 1):\n",
    "            len_batch_train = int(np.ceil(len_batch_train * self.batchsize))\n",
    "            len_batch_test = int(np.ceil(len_batch_test * self.batchsize))\n",
    "        \n",
    "        # Divide training and testing batches\n",
    "        test_output = testing[0:len_batch_test][0:len_batch_test]\n",
    "        inputs = training[0][0:len_batch_train]\n",
    "        targets = np.zeros((len_batch_train, 10))\n",
    "        for i in range(len_batch_train):\n",
    "            targets[i, training[1][i]] = 1\n",
    "        \n",
    "        # Start prints \n",
    "        self.start_time = dt.datetime.now()\n",
    "        print('-- Training Session Start (%s) --' % (self.start_time))\n",
    "        typeTrainingPrint = \"Epochs: \"+str(self.epochs)\n",
    "        if self.epochs == 0:\n",
    "            typeTrainingPrint = \"Until 5 iterations w/o improvements\";\n",
    "        print('\\nNeurons: %d\\nBatch Train: %d\\nBatch Test: %d\\n%s\\n' % (self.neurons,len_batch_train,len_batch_test,typeTrainingPrint))\n",
    "\n",
    "        \n",
    "        # Performs iterations\n",
    "        while (self.epochs == 0 and self.same < 5) or (self.iteration < self.epochs):\n",
    "            \n",
    "            # Backpropagate with feed forward\n",
    "            for input_vector, target_vector in zip(inputs, targets):\n",
    "                # from index matrix to weights matrix with centroid\n",
    "                weights = []\n",
    "                for idxm,c in zip(self.idx_layers,self.centers):\n",
    "                    w = values_from_dict(idxm,c)\n",
    "                    weights.append(w)\n",
    "                self.backpropagate(input_vector, target_vector, weights)\n",
    "                \n",
    "            # Accuracy\n",
    "            accu = self.accu(test_output,weights)\n",
    "            self.iteration += 1\n",
    "            \n",
    "            # Messages\n",
    "            #if (self.iteration == 1 or self.iteration % 10 == 0 or self.iteration == self.epochs):\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "            \n",
    "            # Control coverange\n",
    "            if self.epochs == 0:\n",
    "                if accu[0] > self.best:\n",
    "                    self.same = 0\n",
    "                    self.best = accu[0]\n",
    "                else:\n",
    "                    self.same += 1\n",
    "                \n",
    "        # Print last epoch result\n",
    "        if self.iteration % 10 != 0:\n",
    "            self.print_message_iter(self.iteration,accu,self.ETAepoch(self.start_time))\n",
    "\n",
    "        # Final message\n",
    "        print('\\n-- Training Session End (%s) --' % (dt.datetime.now()))\n",
    "\n",
    "    def feed_forward(self, input_vector, weights):\n",
    "        outputs = []\n",
    "        for w in weights:\n",
    "            input_with_bias = np.append(input_vector, 1)   # Ajout constante\n",
    "            output = np.inner(w, input_with_bias)\n",
    "            output = special.expit(output) # Sigmoid function\n",
    "            outputs.append(output)\n",
    "            # The output is the input of the next layer\n",
    "            input_vector = output\n",
    "        return outputs\n",
    "\n",
    "    def backpropagate(self, input_vector, target, weights):\n",
    "        c = 1./(self.iteration + 100)  # Learning coefficient\n",
    "        hidden_outputs, outputs = self.feed_forward(input_vector, weights)\n",
    "\n",
    "        # Calculation of partial derivatives for the output layer and subtraction\n",
    "        output_deltas = outputs * (1 - outputs) * (outputs - target)\n",
    "        gradient = np.outer(output_deltas, np.append(hidden_outputs, 1))\n",
    "        cg = centroid_gradient(self.idx_layers[-1],gradient,self.cluster)\n",
    "        self.centers[-1] = {x: self.centers[-1][x]-(c*cg[x]) for x in self.centers[-1]}\n",
    "\n",
    "        # Calculation of partial derivatives for the hidden layer and subtraction\n",
    "        hidden_deltas = hidden_outputs * (1 - hidden_outputs) * np.dot(np.delete(weights[-1], 200, 1).T, output_deltas)\n",
    "        gradient = np.outer(hidden_deltas, np.append(input_vector, 1))\n",
    "        cg = centroid_gradient(self.idx_layers[0],gradient,self.cluster)\n",
    "        self.centers[0] = {x: self.centers[0][x]-(c*cg[x]) for x in self.centers[0]}\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, input_vector, weights):\n",
    "        return self.feed_forward(input_vector,weights)[-1]\n",
    "\n",
    "    def predict_one(self, input_vector, weights):\n",
    "        return np.argmax(self.feed_forward(input_vector,weights)[-1])\n",
    "\n",
    "    def accu(self, testing, weights):\n",
    "        res = np.zeros((10, 2))\n",
    "        for k in range(len(testing[1])):\n",
    "            if self.predict_one(testing[0][k], weights) == testing[1][k]:\n",
    "                res[testing[1][k]] += 1\n",
    "            else:\n",
    "                res[testing[1][k]][1] += 1\n",
    "        total = np.sum(res, axis=0)\n",
    "        each = [res[k][0]/res[k][1] for k in range(len(res))]\n",
    "        min_c = sorted(range(len(each)), key=lambda k: each[k])[0]\n",
    "        return np.round([each[min_c]*100, total[0]/total[1]*100, min_c], 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def print_message_iter(self,iteration,accu,eta):\n",
    "        len_eta = len(eta)\n",
    "        space_fill = 6 - len_eta\n",
    "        eta = \"(\"+eta+\")\"\n",
    "        for _ in range(space_fill):\n",
    "            eta += \" \"\n",
    "        message = 'Epoch '+str(self.iteration).zfill(3) + \" \"+eta+\" \"\n",
    "        message += 'Precision G:'+str(accu[1]).zfill(4)+'%\\tMin:'+ str(accu[0]).zfill(4)+ '% ('+str(int(accu[2]))+')'\n",
    "        print(message)\n",
    "        \n",
    "    def getWeights(self):\n",
    "        return self.layers\n",
    "    \n",
    "    def minsec2sec(self,time):\n",
    "        if 'm' in time:\n",
    "            splitted = time.split('m')\n",
    "            return int(splitted[0]) * 60 + int(splitted[1][:-1])\n",
    "        else:\n",
    "            return int(time[:-1])\n",
    "    \n",
    "    def ETAepoch(self,start_time):\n",
    "        diff = dt.datetime.now() - self.start_time\n",
    "        eta = divmod(diff.days * 86400 + diff.seconds, 60)\n",
    "        if eta[0] != 0:\n",
    "            ret = str(eta[0])+\"m\"\n",
    "        else:\n",
    "            ret = \"\"\n",
    "        ret += str(eta[1])+\"s\"\n",
    "        return ret\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting Time: 0m7s ---\n",
      "-- Training Session Start (2018-08-15 12:04:01.464507) --\n",
      "\n",
      "Neurons: 300\n",
      "Batch Train: 600\n",
      "Batch Test: 100\n",
      "Until 5 iterations w/o improvements\n",
      "\n",
      "Epoch 001 (44s)    Precision G:93.41%\tMin:90.66% (8)\n",
      "Epoch 002 (1m28s)  Precision G:93.43%\tMin:90.86% (8)\n",
      "Epoch 003 (2m10s)  Precision G:93.49%\tMin:90.88% (9)\n",
      "Epoch 004 (2m54s)  Precision G:93.46%\tMin:90.88% (9)\n",
      "Epoch 005 (3m37s)  Precision G:93.53%\tMin:90.88% (9)\n",
      "Epoch 006 (4m19s)  Precision G:93.51%\tMin:90.88% (9)\n",
      "Epoch 007 (5m2s)   Precision G:93.57%\tMin:91.08% (9)\n",
      "Epoch 008 (5m45s)  Precision G:93.58%\tMin:91.18% (9)\n",
      "Epoch 009 (6m28s)  Precision G:93.58%\tMin:91.18% (9)\n",
      "Epoch 010 (7m11s)  Precision G:93.58%\tMin:91.08% (9)\n",
      "Epoch 011 (7m54s)  Precision G:93.59%\tMin:90.98% (9)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-df854fb27e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn_km\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeural_Network_KM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn_km\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTESTING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-235-822b39a51683>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training, testing)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0midxm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues_from_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                     \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-32951ed12743>\u001b[0m in \u001b[0;36mvalues_from_dict\u001b[0;34m(a, d)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnew_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnew_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn_km = Neural_Network_KM(neurons=300,epochs=0,batchsize=0.01,cluster=200,pre_weights=nn_weights,verbose=True)\n",
    "nn_km.train(TRAINING,TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_number = 200\n",
    "centers = build_clusters(cluster_number,nn_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map (i,j) -> k\n",
    "def dict_index_to_cluster(weights,centers):\n",
    "        dict_ret = {}\n",
    "        for i, row in enumerate(weights):\n",
    "            for j, col in enumerate(row):\n",
    "                dict_ret[(i,j)] = nearest_centroid_index([x for x in centers.values()],weights[i,j])\n",
    "        return dict_ret\n",
    "\n",
    "dict_index = dict_index_to_cluster(nn_weights[0],centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map k -> (i,j)\n",
    "def dict_cluster_to_index(dict_idx):\n",
    "    dict_ret = {}\n",
    "    for k,v in dict_idx.items():\n",
    "        if v in dict_ret:\n",
    "            dict_ret[v] += [k]\n",
    "        else:\n",
    "            dict_ret[v] = [k]\n",
    "    return dict_ret\n",
    "\n",
    "dict_cluster = dict_cluster_to_index(dict_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 69),\n",
       " (0, 93),\n",
       " (0, 159),\n",
       " (0, 284),\n",
       " (0, 366),\n",
       " (0, 397),\n",
       " (0, 597),\n",
       " (1, 92),\n",
       " (1, 93),\n",
       " (1, 118)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helper function\n",
    "def getCluster(d,v):\n",
    "    return d[v]\n",
    "\n",
    "def getIndices(d,k):\n",
    "    return d[k]\n",
    "\n",
    "getCluster(dict_index,(7,42))\n",
    "getIndices(dict_cluster,106)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24180321e-03,  3.09352056e-03,  2.40312991e-03, ...,\n",
       "         8.24244592e-04,  1.11358114e-03,  8.35011357e-02],\n",
       "       [ 3.09352056e-03,  1.86535973e-03,  1.32508599e-03, ...,\n",
       "         3.58020812e-04,  2.06570544e-03,  1.07167750e-01],\n",
       "       [ 1.86535973e-03,  5.85474138e-04,  2.40312991e-03, ...,\n",
       "         2.06570544e-03,  2.95705729e-03, -1.48669266e-02],\n",
       "       ...,\n",
       "       [ 1.67534221e-03,  2.40312991e-03,  1.67534221e-03, ...,\n",
       "         3.09352056e-03,  1.11358114e-03,  5.72519726e-03],\n",
       "       [ 3.09352056e-03,  6.62278075e-05,  1.86535973e-03, ...,\n",
       "         3.27627643e-03,  4.80584123e-04,  5.21549707e-02],\n",
       "       [ 3.09352056e-03,  6.62278075e-05,  1.11358114e-03, ...,\n",
       "         1.86535973e-03,  3.09352056e-03,  1.13152861e-01]])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from dict (k -> (i,j)) to matrix with centers\n",
    "def dict_to_matrix(dict_index,dict_values):\n",
    "    coord_array = np.asarray(list(dict_index.values()))\n",
    "    values_array = np.asarray(list(dict_values.values()))\n",
    "    return values_array[coord_array].reshape(300,785)\n",
    "\n",
    "dict_to_matrix(dict_index,centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.24180321e-03,  3.09352056e-03,  2.40312991e-03, ...,\n",
       "         8.24244592e-04,  1.11358114e-03,  8.35011357e-02],\n",
       "       [ 3.09352056e-03,  1.86535973e-03,  1.32508599e-03, ...,\n",
       "         3.58020812e-04,  2.06570544e-03,  1.07167750e-01],\n",
       "       [ 1.86535973e-03,  5.85474138e-04,  2.40312991e-03, ...,\n",
       "         2.06570544e-03,  2.95705729e-03, -1.48669266e-02],\n",
       "       ...,\n",
       "       [ 1.67534221e-03,  2.40312991e-03,  1.67534221e-03, ...,\n",
       "         3.09352056e-03,  1.11358114e-03,  5.72519726e-03],\n",
       "       [ 3.09352056e-03,  6.62278075e-05,  1.86535973e-03, ...,\n",
       "         3.27627643e-03,  4.80584123e-04,  5.21549707e-02],\n",
       "       [ 3.09352056e-03,  6.62278075e-05,  1.11358114e-03, ...,\n",
       "         1.86535973e-03,  3.09352056e-03,  1.13152861e-01]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# old function with index matrix\n",
    "idx_mat = redefine_weights(nn_weights[0],[v for v in centers.values()])\n",
    "values_from_dict(idx_mat,centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.4 ms ± 1.3 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "25.3 ms ± 167 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit values_from_dict(idx_mat,centers)\n",
    "%timeit dict_to_matrix(dict_index,centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.7 ms ± 378 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit centroid_gradient(idx_mat,nn_weights[0],cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 ms ± 3.06 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "def cg_1(d_idx,m_wei,cluster):\n",
    "    ret = [0 for x in range(cluster)]\n",
    "    for k,v in d_idx.items():\n",
    "        for i in v:\n",
    "            ret[k] += m_wei[i]\n",
    "    return ret\n",
    "\n",
    "%timeit cg_1(dict_cluster,nn_weights[0],cluster_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
